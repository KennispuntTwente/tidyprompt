---
title: "Getting started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
resource_files:
  - ../man/figures/answer_using_r1-1.png
---

```{r chunk_options, include = FALSE}

here::i_am("vignettes/getting_started.Rmd")

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/"
)

options(tidyprompt.verbose = TRUE)
options(tidyprompt.stream = FALSE)

```

```{r setup, results = "hide"}
library(tidyprompt)
```

### Setup an LLM provider

'tidyprompt' can be used with any LLM provider capable of completing a chat.

At the moment, 'tidyprompt' includes pre-built functions to connect with various
LLM providers, such as Ollama, OpenAI, OpenRouter, Mistral, Groq, XAI (Grok), and Google Gemini.

With the `llm_provider-class`, you can easily write a hook for any other LLM provider.
You could make API calls using the 'httr2' package or use another R package that already has
a hook for the LLM provider you want to use. If your API of choice follows the structure
of the OpenAI API, you can call `llm_provider_openai()` and change
the relevant parameters (like the URL and the API key).

```{r llm_provider}
# Ollama running on local PC
ollama <- llm_provider_ollama(
  parameters = list(model = "llama3.1:8b"),
)

# OpenAI API
openai <- llm_provider_openai(
  parameters = list(model = "gpt-4o-mini")
)

# Various providers via OpenRouter (e.g., Anthropic)
openrouter <- llm_provider_openrouter(
  parameters = list(model = "anthropic/claude-3.5-sonnet")
)

# ... functions also included for Mistral, Groq, XAI (Grok), and Google Gemini

# ... or easily create your own hook for any other LLM provider;
#   see ?llm_provider-class for more information; also take a look at the source code of
#   llm_provider_ollama() and llm_provider_openai(). For APIs that follow the structure
#   of the OpenAI API for chat completion, you can use llm_provider_openai()
```

### Basic prompting

A simple string serves as the base for a prompt.

By adding prompt wraps, you can influence various aspects of how the LLM handles the prompt,
while verifying that the output is structured and valid (including retries with feedback to the LLM if it is not).

```{r prompt_example1, cache=FALSE}
  "Hi there!" |>
    send_prompt(ollama)
```
`add_text()` is a simple example of a prompt wrap. It simply adds some text at the end of the base prompt.

```{r prompt_example2, cache=FALSE}
  "Hi there!" |>
    add_text("What is a large language model? Explain in 10 words.") |>
    send_prompt(ollama)
```
You can also construct the final prompt text, without sending it to an LLM provider.

```{r prompt_example3, cache=FALSE}
  "Hi there!" |>
    add_text("What is a large language model? Explain in 10 words.")
```

### Retrieving output in a specific format

Using prompt wraps, you can force the LLM to return the output in a specific format.
You can also extract the output to turn it from a character into another data type.

For instance, `answer_as_integer()` adds a prompt wrap which forces the LLM
to reply with an integer.

To achieve this, the prompt wrap will add some text to the base prompt, asking
the LLM to reply with an integer. However, the prompt wrap does more: it also
will attempt to extract and validate the integer from the LLM's response. If
extraction or validation fails, feedback is sent back to the LLM, after which
the LLM can retry answering the prompt. Because the extraction function turns the
original character response into a numeric value, the final output from
`send_prompt()` will also be a numeric type.

```{r answer_as_integer1, cache=FALSE}
  "What is 2 + 2?" |>
    answer_as_integer() |>
    send_prompt(ollama)
```
Below is an example of a prompt which will initially fail, but will succeed after
`llm_feedback()` and a retry.

```{r answer_as_integer2, cache=FALSE}
  "What is 2 + 2?" |>
    add_text("Please write out your reply in words, use no numbers.") |>
    answer_as_integer(add_instruction_to_prompt = FALSE) |>
    send_prompt(ollama)
```

'tidyprompt' offers various other 'answer_as' functions, such as `answer_as_boolean()`, `answer_as_regex()`,
`answer_as_named_list()`, `answer_using_r()`, and `answer_as_json()`.

##### JSON output

`answer_as_json()` may be especially powerful when your LLM provider and model have native support for
returning JSON objects and adhering to JSON schemas (e.g., OpenAI, Ollama). Text-based handling is however always
possible, also for providers which do not natively support such functions. This means that
you can always switch between providers while ensuring the results will be in the correct format.

It should also be noted that native JSON enforcement may also restrict the model too much,
so it is always good to test and see what works best for your use case. In our experience,
text-based handling is often more flexible and robust, especially when combining
multiple prompt wraps. Note also that JSON schemas may not enforce all qualities
you are looking for in the output, so you may still need to add additional prompt wraps
to ensure the output is as desired.

### Adding a reasoning mode to the LLM

Prompt wraps may also be used to add a reasoning mode to the LLM. It is hypothesized that this could
improve the LLM's performance on more complex tasks.

For instance, `answer_by_chain_of_thought()` will add chain of thought reasoning mode to the
prompt evaluation by the LLM.
The function wraps the base prompt text within a request for the LLM to reason step by step, asking
it to provide the final answer within 'FINISH[<final answer here>]'. An extraction
function then ensures only the final answer is returned.

```{r mode_cot, cache=FALSE}
  "What is 2 + 2?" |>
    answer_by_chain_of_thought() |>
    answer_as_integer() |>
    send_prompt(ollama)
```

### Giving tools to the LLM (autonomous function-calling)

With `answer_using_tools()`, you can enable your LLM to call R functions.
This enables the LLM to autonomously retrieve additional information or take other actions.

`answer_using_tools()` automatically extracts documentation when it is available
for base R functions and functions from packages. Types are inferred from the
default arguments of the function. If you want to define a custom function and/or override the default documentation,
you can use `tools_add_docs()`. See example usage in the documentation
of `answer_using_tools()`.

`answer_using_tools()` supports both text-based function calling and native function calling (via API
parameters, currently implemented for OpenAI and Ollama API structures).

```{r tool_example2, cache=FALSE}
  "What are the files in my current directory?" |>
    answer_using_tools(list.files) |>
    send_prompt(ollama)
```

### Code generation and evaluation

`answer_using_r()` provides a more advanced prompt wrap, which has various options
to enable LLM code generation. R code can be extracted, parsed
for validity, and optionally be evaluated in a dedicated R session (using the
'callr' package). The prompt wrap can also be set to 'tool mode' (with `output_as_tool = TRUE`),
where the output of R code is returned to the LLM, so that it can be used to formulate
a final answer.

```{r include=FALSE}
openai$verbose <- FALSE; options(tidyprompt.verbose = FALSE)
```

```{r answer_using_r1, warning=FALSE, cache=TRUE, eval=FALSE}
# From prompt to ggplot
plot <- paste0(
  "Create a scatter plot of miles per gallon (mpg) versus",
  " horsepower (hp) for the cars in the mtcars dataset.",
  " Use different colors to represent the number of cylinders (cyl).",
  " Make the plot nice and readable,",
  " but also be creative, a little crazy, and have humour!"
) |>
  answer_using_r(
    pkgs_to_use = c("ggplot2"),
    evaluate_code = TRUE,
    return_mode = "object"
  ) |>
  send_prompt(openai)
plot
```
![plot](figure/answer_using_r1-1.png)

```{r include=FALSE}
openai$verbose <- TRUE; options(tidyprompt.verbose = TRUE)
```

### Creating prompt wraps

See `prompt_wrap()` and the '[Creating prompt wraps](vignette("data-preparation"))'
vignette for information on how to create your own prompt wraps.
