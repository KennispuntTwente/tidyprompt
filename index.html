<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Prompt Large Language Models (LLMs) And Enhance Their Functionality • tidyprompt</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Prompt Large Language Models (LLMs) And Enhance Their Functionality">
<meta name="description" content="‘tidyprompt’ is an R package to easily prompt large language models (LLMs) and enhance their functionality. A framework is provided which allows users to construct LLM prompts using tidyverse-inspired piping syntax. Prompt wraps influence how a LLM handles the base prompt, through modifying the original text and applying extraction and validation functions to the LLM response. Users can choose from a library of pre-built prompt wraps or write their own. This allows for obtaining structured LLM output, with automatic feedback and retries if necessary. Additionally, the package supports LLM reasoning modes (e.g., chain of thought), autonomous R function calling, and R code generation and evaluation. tidyprompt is compatible with any LLM provider that offers chat completions.">
<meta property="og:description" content="‘tidyprompt’ is an R package to easily prompt large language models (LLMs) and enhance their functionality. A framework is provided which allows users to construct LLM prompts using tidyverse-inspired piping syntax. Prompt wraps influence how a LLM handles the base prompt, through modifying the original text and applying extraction and validation functions to the LLM response. Users can choose from a library of pre-built prompt wraps or write their own. This allows for obtaining structured LLM output, with automatic feedback and retries if necessary. Additionally, the package supports LLM reasoning modes (e.g., chain of thought), autonomous R function calling, and R code generation and evaluation. tidyprompt is compatible with any LLM provider that offers chat completions.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="articles/creating_prompt_wraps.html">Creating prompt wraps</a></li>
    <li><a class="dropdown-item" href="articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjarkvandemerwe/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="tidyprompt">tidyprompt<a class="anchor" aria-label="anchor" href="#tidyprompt"></a>
</h1></div>
<!-- badges: start -->

<p>‘tidyprompt’ is an R package to easily prompt large language models (‘LLMs’) and enhance their functionality.</p>
<p>Key features of ‘tidyprompt’ are:</p>
<ul>
<li><p><strong>tidy prompting</strong>: Easily construct prompts for LLMs, using piping syntax (inspired by the ‘tidyverse’). Prompt wraps are building blocks with which you can quickly create advanced prompts, simultaneously adding extraction and validation functions for processing the LLM output. A library of pre-built prompt wraps is included, but you can also write custom prompt wraps.</p></li>
<li><p><strong>structured output</strong>: Obtain structured output from a LLM, adhering to a specific type and/or format.</p></li>
<li><p><strong>feedback &amp; retries</strong>: Automatically provide feedback to the LLM when the output is not as expected, so that it can retry.</p></li>
<li><p><strong>reasoning modes</strong>: Make your LLM answer a prompt in a specific mode, such as chain-of-thought or ReAct (Reasoning and Acting) modes.</p></li>
<li><p><strong>function calling</strong>: Give your LLM the ability to autonomously call R functions (‘tools’). With this, the LLM can retrieve information or take other actions. ‘tidyprompt’ also supports R code generation and evaluation, allowing LLMs to run R code.</p></li>
<li><p><strong>compatible with all LLM providers</strong>: All features of ‘tidyprompt’ are designed to be provider-agnostic, meaning that they can be used with any LLM provider that supports chat completion. ‘tidyprompt’ includes various default LLM providers, including Ollama, OpenAI, OpenRouter (offering various providers, including Anthropic), Mistral, Groq, XAI (Grok), and Google Gemini. You can also write a hook for any other LLM provider.</p></li>
</ul>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install the development version of tidyprompt from <a href="https://github.com/tjarkvandemerwe/tidyprompt" class="external-link">GitHub</a> with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("remotes")</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"tjarkvandemerwe/tidyprompt"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<p>See the <a href="https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html">‘Getting started’</a> vignette for a detailed introduction to using ‘tidyprompt’.</p>
<p>Below are some quick examples of what is possible with ‘tidyprompt’:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 5+5?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 10</span></span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="st">"Are you a large language model?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_boolean.html">answer_as_boolean</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="st">"What animal is the biggest?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_regex.html">answer_as_regex</a></span><span class="op">(</span><span class="st">"^(cat|dog|elephant)$"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "elephant"</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make LLM use a function from an R package to search Wikipedia for the answer</span></span>
<span><span class="st">"What is something fun that happened in November 2024?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"Summarize in one sentence."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_using_tools.html">answer_using_tools</a></span><span class="op">(</span><span class="fu">getwiki</span><span class="fu">::</span><span class="va">search_wiki</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "The 2024 ARIA Music Awards ceremony, a vibrant celebration of Australian music,</span></span>
<span><span class="co">#&gt; took place on November 20, 2024."</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># From prompt to linear model object in R</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span></span>
<span>  <span class="st">"Using my data, create a statistical model"</span>,</span>
<span>  <span class="st">" investigating the relationship between two variables."</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_code.html">answer_as_code</a></span><span class="op">(</span></span>
<span>    objects_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">cars</span><span class="op">)</span>,</span>
<span>    evaluate_code <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    return_mode <span class="op">=</span> <span class="st">"object"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span></span>
<span>    validation_fn <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">inherits</a></span><span class="op">(</span><span class="va">x</span>, <span class="st">"lm"</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_feedback.html">llm_feedback</a></span><span class="op">(</span><span class="st">"The output should be a linear model object."</span><span class="op">)</span><span class="op">)</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = speed ~ dist, data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -7.5293 -2.1550  0.3615  2.4377  6.4179 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  8.28391    0.87438   9.474 1.44e-12 ***</span></span>
<span><span class="co">#&gt; dist         0.16557    0.01749   9.464 1.49e-12 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 3.156 on 48 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 </span></span>
<span><span class="co">#&gt; F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="more-information-and-contributing">More information and contributing<a class="anchor" aria-label="anchor" href="#more-information-and-contributing"></a>
</h2>
<p>‘tidyprompt’ is under active development by Luka Koning (<a href="mailto:l.koning@kennispunttwente.nl" class="email">l.koning@kennispunttwente.nl</a>) and Tjark van de Merwe (<a href="mailto:t.vandemerwe@kennispunttwente.nl" class="email">t.vandemerwe@kennispunttwente.nl</a>). Note that in this stage, the package is not yet fully stable and its architecture is subject to change.</p>
<p>If you encounter issues, please open an issue in the GitHub repository. You are welcome to contribute to the package by opening a pull request. If you have any questions or suggestions, you can also reach us via e-mail.</p>
<div class="section level3">
<h3 id="philosophy-of-tidyprompt">Philosophy of ‘tidyprompt’<a class="anchor" aria-label="anchor" href="#philosophy-of-tidyprompt"></a>
</h3>
<p>‘tidyprompt’ should be seen as a tool which can be used to enhance the functionality of LLMs beyond what APIs natively offer. It is designed to be flexible and provider-agnostic, so that its features can be used with a wide range of LLM providers and models.</p>
<p>‘tidyprompt’ is primarily focused on ‘text-based’ handling of LLMs, where textual output is parsed to achieve structured output and other functionalities. Several LLM providers and models also offers forms of ‘native’ handling, where the LLM is directly controlled by the LLM provider to provide output in a certain manner.</p>
<p>Where appropriate, ‘tidyprompt’ may also support native configuration of specific APIs. Currently, <code><a href="reference/answer_as_json.html">answer_as_json()</a></code> and <code><a href="reference/answer_using_tools.html">answer_using_tools()</a></code> offer native support for adhering to JSON schemas and calling functions. Native handling may be powerful in some cases, but restrictive in other cases. It is good to test what works best for your use case. Note also that prompt wraps may extend what is enforced by native handling, such as adding additional validation or feedback.</p>
<p>The philosophy behind ‘tidyprompt’ is furthermore that it aims to be flexible enough that users can implement advanced features, potentially specific to certain LLM providers, within the options of their custom prompt wraps. This way, ‘tidyprompt’ can be a powerful tool for a wide range of use cases, without focusing on maintaining provider-specific features.</p>
<div class="section level4">
<h4 id="tidyprompt-versus-elmer--tidyllm">‘tidyprompt’ versus ‘elmer’ &amp; ‘tidyllm’<a class="anchor" aria-label="anchor" href="#tidyprompt-versus-elmer--tidyllm"></a>
</h4>
<p>In line with the above philosophy, ‘tidyprompt’ is less focused on interfacing with the APIs of various LLM providers, like R packages ‘elmer’ and ‘tidyllm’ do. Instead, ‘tidyprompt’ is primarily focused on offering a framework for constructing prompts and associated logic for complex interactions with LLMs.</p>
<p>We aim to design ‘tidyprompt’ in such a way that it may be compatible with ‘elmer’, ‘tidyllm’, and any other packages offering an interface to LLM APIs. We are open to feedback on our design and may include compatability with specific features from these packages in the future.</p>
</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/tjarkvandemerwe/tidyprompt/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/tjarkvandemerwe/tidyprompt/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3) | file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing tidyprompt</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Luka Koning <br><small class="roles"> Author, maintainer, copyright holder </small>  </li>
<li>Tjark Van de Merwe <br><small class="roles"> Author, copyright holder </small>  </li>
<li>Kennispunt Twente <br><small class="roles"> Funder </small>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/tjarkvandemerwe/tidyprompt/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/tjarkvandemerwe/tidyprompt/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
