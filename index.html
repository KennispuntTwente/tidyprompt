<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Prompt Large Language Models and Enhance Their Functionality • tidyprompt</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Prompt Large Language Models and Enhance Their Functionality">
<meta name="description" content="Easily construct prompts and associated logic for interacting with large language models (LLMs). tidyprompt introduces the concept of prompt wraps, which are building blocks that you can use to quickly turn a simple prompt into a complex one. Prompt wraps do not just modify the prompt text, but also add extraction and validation functions that will be applied to the response of the LLM. This ensures that the user gets the desired output. tidyprompt can add various features to prompts and their evaluation by LLMs, such as structured output, automatic feedback, retries, reasoning modes, autonomous R function calling, and R code generation and evaluation. It is designed to be compatible with any LLM provider that offers chat completion.">
<meta property="og:description" content="Easily construct prompts and associated logic for interacting with large language models (LLMs). tidyprompt introduces the concept of prompt wraps, which are building blocks that you can use to quickly turn a simple prompt into a complex one. Prompt wraps do not just modify the prompt text, but also add extraction and validation functions that will be applied to the response of the LLM. This ensures that the user gets the desired output. tidyprompt can add various features to prompts and their evaluation by LLMs, such as structured output, automatic feedback, retries, reasoning modes, autonomous R function calling, and R code generation and evaluation. It is designed to be compatible with any LLM provider that offers chat completion.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="articles/creating_prompt_wraps.html">Creating prompt wraps</a></li>
    <li><a class="dropdown-item" href="articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/KennispuntTwente/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="tidyprompt">tidyprompt<a class="anchor" aria-label="anchor" href="#tidyprompt"></a>
</h1></div>
<!-- badges: start -->

<p>‘tidyprompt’ is an R package to easily construct prompts and associated logic for interacting with large language models (‘LLMs’).</p>
<p>Think of ‘tidyprompt’ as the ‘ggplot2’ package for creating prompts and handling LLM interactions. ‘tidyprompt’ introduces the concept of prompt wraps, which are building blocks that you can use to quickly turn a simple prompt into an advanced one. Prompt wraps do not just modify the prompt text, but also add extraction and validation functions that will be applied to the response of a LLM. Moreover, these functions can send feedback to the LLM.</p>
<p>With ‘tidyprompt’ and prompt wraps, you can add various features to your prompts and define how they are evaluated by LLMs. For example:</p>
<ul>
<li><p><strong>structured output</strong>: Obtain structured output from a LLM, adhering to a specific type and/or format. Use pre-built prompt wraps or your own R code to validate.</p></li>
<li><p><strong>feedback &amp; retries</strong>: Automatically provide feedback to a LLM when the output is not as expected, allowing the LLM to retry their answer.</p></li>
<li><p><strong>reasoning modes</strong>: Make a LLM answer a prompt in a specific mode, such as chain-of-thought or ReAct (Reasoning and Acting).</p></li>
<li><p><strong>function calling</strong>: Give a LLM the ability to autonomously call R functions (‘tools’). With this, the LLM can retrieve information or take other actions. ‘tidyprompt’ also supports R code generation and evaluation, allowing LLMs to run R code. Tools from Model Context Protocol (MCP) servers are also supported</p></li>
</ul>
<p>With its features, ‘tidyprompt’ extends the functionality of LLMs beyond what is natively offered by LLM APIs, and you can elegantly design complex, robust interactions with LLMs.</p>
<p><em>‘tidyprompt’ is compatible with the <a href="https://ellmer.tidyverse.org/index.html" class="external-link">‘ellmer’</a> R package, which has gained popularity for interfacing with LLM APIs. ‘tidyprompt’ supports connecting to LLM providers from ‘ellmer’ chat objects, and ‘ellmer’ definitions for structured output and tools (see <a href="#tidyprompt-versus-ellmer--tidyllm">more information below</a>).</em></p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>Install the development version from GitHub:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("remotes")</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"KennispuntTwente/tidyprompt"</span><span class="op">)</span></span></code></pre></div>
<p>Or install from CRAN (0.0.1):</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"tidyprompt"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<p>See the <a href="https://KennispuntTwente.github.io/tidyprompt/articles/getting_started.html" class="external-link">‘Getting started’</a> vignette for a detailed introduction to using ‘tidyprompt’.</p>
</div>
<div class="section level2">
<h2 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h2>
<p>Here are some quick examples of what you can do with ‘tidyprompt’:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="st">"What is 5+5?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 10</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="st">"Are you a large language model?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_boolean.html">answer_as_boolean</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="st">"What animal is the biggest?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_regex_match.html">answer_as_regex_match</a></span><span class="op">(</span><span class="st">"^(cat|dog|elephant)$"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "elephant"</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make LLM use a function from an R package to search Wikipedia for the answer</span></span>
<span><span class="st">"What is something fun that happened in November 2024?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_text.html">answer_as_text</a></span><span class="op">(</span>max_words <span class="op">=</span> <span class="fl">25</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_using_tools.html">answer_using_tools</a></span><span class="op">(</span><span class="fu">getwiki</span><span class="fu">::</span><span class="va">search_wiki</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "The 2024 ARIA Music Awards ceremony, a vibrant celebration of Australian music,</span></span>
<span><span class="co">#&gt; took place on November 20, 2024."</span></span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># From prompt to linear model object in R</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span></span>
<span>  <span class="st">"Using my data, create a statistical model"</span>,</span>
<span>  <span class="st">" investigating the relationship between two variables."</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_using_r.html">answer_using_r</a></span><span class="op">(</span></span>
<span>    objects_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">cars</span><span class="op">)</span>,</span>
<span>    evaluate_code <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    return_mode <span class="op">=</span> <span class="st">"object"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span></span>
<span>    validation_fn <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">inherits</a></span><span class="op">(</span><span class="va">x</span>, <span class="st">"lm"</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_feedback.html">llm_feedback</a></span><span class="op">(</span><span class="st">"The output should be a linear model object."</span><span class="op">)</span><span class="op">)</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = speed ~ dist, data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -7.5293 -2.1550  0.3615  2.4377  6.4179 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  8.28391    0.87438   9.474 1.44e-12 ***</span></span>
<span><span class="co">#&gt; dist         0.16557    0.01749   9.464 1.49e-12 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 3.156 on 48 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 </span></span>
<span><span class="co">#&gt; F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Escape validation on questions that cannot be answered</span></span>
<span><span class="st">"How many years old is my neighbour's dog?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/quit_if.html">quit_if</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; NULL</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># LLM in the loop; </span></span>
<span><span class="co">#   LLM verifies answer of LLM and can provide feedback</span></span>
<span><span class="st">"What is the capital of France?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_verify.html">llm_verify</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ...</span></span>
<span>  </span>
<span><span class="co"># Human in the loop; </span></span>
<span><span class="co">#   user verifies answer of LLM and can provide feedback</span></span>
<span><span class="st">"What is the capital of France?"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/user_verify.html">user_verify</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ...</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="more-information-and-contributing">More information and contributing<a class="anchor" aria-label="anchor" href="#more-information-and-contributing"></a>
</h2>
<p>‘tidyprompt’ is developed by Luka Koning (<a href="mailto:l.koning@kennispunttwente.nl" class="email">l.koning@kennispunttwente.nl</a>) and Tjark van de Merwe (<a href="mailto:t.vandemerwe@kennispunttwente.nl" class="email">t.vandemerwe@kennispunttwente.nl</a>).</p>
<p>If you encounter issues, have questions, or have suggestions, please open an issue in the GitHub repository. You are also welcome to contribute to the package by opening a pull request.</p>
<div class="section level3">
<h3 id="why-tidyprompt">Why ‘tidyprompt’?<a class="anchor" aria-label="anchor" href="#why-tidyprompt"></a>
</h3>
<p>We designed ‘tidyprompt’ because we found ourselves writing code repeatedly to both construct prompts and handle the associated output of LLMs; these tasks were intertwined. Often times, we also wanted to add features to our prompts, or take them away, which required us to rewrite a lot of code. Thus, we wanted to have building blocks with which we could easily construct prompts and simultaneously add code to handle the output of LLMs. This led us to a design inspired by piping syntax, as popularized by the ‘tidyverse’ and familiar to many R users.</p>
<p>‘tidyprompt’ should be seen as a tool which can be used to enhance the functionality of LLMs beyond what APIs natively offer. It is designed to be flexible and provider-agnostic, so that its features can be used with a wide range of LLM providers and models.</p>
<p>‘tidyprompt’ is primarily focused on ‘text-based’ handling of LLMs, where textual output is parsed to achieve structured output and other functionalities. Several LLM providers and models also offers forms of ‘native’ handling, where the LLM is directly controlled by the LLM provider to provide output in a certain manner.</p>
<p>Where appropriate, ‘tidyprompt’ may also support native configuration of specific APIs. Currently, <code><a href="reference/answer_as_json.html">answer_as_json()</a></code> and <code><a href="reference/answer_using_tools.html">answer_using_tools()</a></code> offer native support for adhering to JSON schemas and calling functions. Native handling may be powerful in some cases, but restrictive in other cases. It is good to test what works best for your use case. Note also that prompt wraps may extend what is enforced by native handling, such as adding additional validation or feedback.</p>
<p>The philosophy behind ‘tidyprompt’ is furthermore that it aims to be flexible enough that users can implement advanced features, potentially specific to certain LLM providers, within the options of their custom prompt wraps. This way, ‘tidyprompt’ can be a powerful tool for a wide range of use cases, without focusing on maintaining provider-specific features.</p>
<div class="section level4">
<h4 id="tidyprompt-versus-ellmer--tidyllm">‘tidyprompt’ versus ‘ellmer’ &amp; ‘tidyllm’<a class="anchor" aria-label="anchor" href="#tidyprompt-versus-ellmer--tidyllm"></a>
</h4>
<p>In line with the above, ‘tidyprompt’ is less focused on interfacing with the APIs of various LLM providers, like R packages ‘ellmer’ and ‘tidyllm’ do. Instead, ‘tidyprompt’ is primarily focused on offering a framework for constructing prompts and associated logic for interactions with LLMs.</p>
<p>We aim to design ‘tidyprompt’ in such a way that it can be compatible with ‘ellmer’, ‘tidyllm’, and any other packages offering an interface to LLM APIs.</p>
<p>The <code><a href="reference/llm_provider_ellmer.html">tidyprompt::llm_provider_ellmer()</a></code> function can create a LLM provider from an <code><a href="https://ellmer.tidyverse.org/reference/chat-any.html" class="external-link">ellmer::chat()</a></code> object, allowing users to use any LLM provider that can be configured with ‘ellmer’, including the respective configuration and features. Furthermore, <code><a href="reference/answer_as_json.html">answer_as_json()</a></code> supports specifying a schema as ‘ellmer’ structured data types (e.g., <code><a href="https://ellmer.tidyverse.org/reference/type_boolean.html" class="external-link">ellmer::type_object()</a></code>). When using an ‘ellmer’ LLM provider, <code><a href="reference/answer_as_json.html">answer_as_json()</a></code> also calls the native ‘ellmer’ chat function for obtaining structed output. Similarly, <code><a href="reference/answer_using_tools.html">answer_using_tools()</a></code> supports ‘ellmer’ tool definitions created by <code><a href="https://ellmer.tidyverse.org/reference/create_tool_def.html" class="external-link">ellmer::create_tool_def()</a></code>, and when using an ‘ellmer’ LLM provider it will use the native ‘ellmer’ chat function for registering tools. (Because <code><a href="https://posit-dev.github.io/mcptools/reference/client.html" class="external-link">mcptools::mcp_tools()</a></code> returns ‘ellmer’ tool definitions, <code><a href="reference/answer_using_tools.html">answer_using_tools()</a></code> also supports the tools from MCP servers.)</p>
</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=tidyprompt" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/KennispuntTwente/tidyprompt/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/KennispuntTwente/tidyprompt/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3) | file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing tidyprompt</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Luka Koning <br><small class="roles"> Author, maintainer, copyright holder </small>   </li>
<li>Tjark Van de Merwe <br><small class="roles"> Author, copyright holder </small>   </li>
<li>Kennispunt Twente <br><small class="roles"> Funder </small>   </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/KennispuntTwente/tidyprompt/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/KennispuntTwente/tidyprompt/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://cran.r-project.org/package=tidyprompt" class="external-link"><img src="https://www.r-pkg.org/badges/version/tidyprompt"></a></li>
<li><a href="https://github.com/KennispuntTwente/tidyprompt" class="external-link"><img src="https://img.shields.io/badge/devel%20version-0.1.0.9000-blue.svg"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
