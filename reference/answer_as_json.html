<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Make LLM answer as JSON — answer_as_json • tidyprompt</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Make LLM answer as JSON — answer_as_json"><meta name="description" content='This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
The function can work with all models and providers when using type "text-based",
but also supports native settings for the OpenAI API and Ollama (the latter
being more efficient and powerful, though specific certain providers and models).
For type "text-based" and "ollama", the JSON schema will be validated
within the extraction function using the jsonvalidate package. For "openai",
the schema will be added to the API request parameters and the API will ensure
the response matches the schema.'><meta property="og:description" content='This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
The function can work with all models and providers when using type "text-based",
but also supports native settings for the OpenAI API and Ollama (the latter
being more efficient and powerful, though specific certain providers and models).
For type "text-based" and "ollama", the JSON schema will be validated
within the extraction function using the jsonvalidate package. For "openai",
the schema will be added to the API request parameters and the API will ensure
the response matches the schema.'></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="../articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjarkvandemerwe/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Make LLM answer as JSON</h1>
      <small class="dont-index">Source: <a href="https://github.com/tjarkvandemerwe/tidyprompt/blob/main/R/answer_as_json.R" class="external-link"><code>R/answer_as_json.R</code></a></small>
      <div class="d-none name"><code>answer_as_json.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
The function can work with all models and providers when using type "text-based",
but also supports native settings for the OpenAI API and Ollama (the latter
being more efficient and powerful, though specific certain providers and models).
For type "text-based" and "ollama", the JSON schema will be validated
within the extraction function using the <a href="https://cran.r-project.org/web/packages/jsonvalidate/index.html" class="external-link">jsonvalidate</a> package. For "openai",
the schema will be added to the API request parameters and the API will ensure
the response matches the schema.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">answer_as_json</span><span class="op">(</span></span>
<span>  <span class="va">prompt</span>,</span>
<span>  type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"text-based"</span>, <span class="st">"ollama"</span>, <span class="st">"openai"</span><span class="op">)</span>,</span>
<span>  schema <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  schema_strict <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  schema_in_prompt_as <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"example"</span>, <span class="st">"schema"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-prompt">prompt<a class="anchor" aria-label="anchor" href="#arg-prompt"></a></dt>
<dd><p>A single string or a <code><a href="tidyprompt.html">tidyprompt()</a></code> object</p></dd>


<dt id="arg-type">type<a class="anchor" aria-label="anchor" href="#arg-type"></a></dt>
<dd><p>The way in which the JSON response will be enforced.
If using "text-based" (default), the JSON response will be extracted from the LLM response
and validated within an extraction function.
If using "openai", relevant parameters will be added to the prompt through which
the API will ensure the response is a valid JSON object. When not providing
a schema, a request for a JSON object is added to the prompt as is required
by the OpenAI API.
If using "ollama", the parameter 'format' will be set to 'json' in the API request
(enforcing a JSON response). Besides that, the handling is the same as "text-based".
As recommended by Ollama, an request for a JSON object is added to the prompt.
Note that Ollama does not yet have native support for JSON schema validation, so the schema will be
validated within the extraction function by the 'jsonvalidate' package.
"text-based" is more generally applicable, while "openai" may be more efficient but
specific to the conditions of the API (note that APIs besides the OpenAI API
may follow the same structure, in which case "openai" may also be used for those APIs)</p></dd>


<dt id="arg-schema">schema<a class="anchor" aria-label="anchor" href="#arg-schema"></a></dt>
<dd><p>(optional) A list representing a JSON schema object that the response must match.
If provided, when using "text-based", the schema will be added to the original prompt and
the response will be validated against the schema with the 'jsonvalidate' package.
If using "openai", the schema will be added to the API request parameters and
the API will ensure the response matches the schema. See examples and/or the OpenAI API
documentation for more information on defining JSON schemas</p></dd>


<dt id="arg-schema-strict">schema_strict<a class="anchor" aria-label="anchor" href="#arg-schema-strict"></a></dt>
<dd><p>(optional) If TRUE, the schema will be strictly enforced.
This option is passed the the API when using 'openai' and to the 'jsonvalidate::json_validate()'
function when using 'text-based'</p></dd>


<dt id="arg-schema-in-prompt-as">schema_in_prompt_as<a class="anchor" aria-label="anchor" href="#arg-schema-in-prompt-as"></a></dt>
<dd><p>(optional) If providing a schema, and when using "text-based" or "ollama",
this argument specifies how the schema should be included in the prompt. If "example" (default),
the schema will be included as an example JSON object. If "schema", the schema will be included
as a JSON schema. "example" typically appears to work better</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A <code><a href="tidyprompt.html">tidyprompt()</a></code> with an added <code><a href="prompt_wrap.html">prompt_wrap()</a></code> which will ensure
that the LLM response is a valid JSON object</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>Other pre_built_prompt_wraps:
<code><a href="add_text.html">add_text</a>()</code>,
<code><a href="add_tools.html">add_tools</a>()</code>,
<code><a href="answer_as_boolean.html">answer_as_boolean</a>()</code>,
<code><a href="answer_as_code.html">answer_as_code</a>()</code>,
<code><a href="answer_as_integer.html">answer_as_integer</a>()</code>,
<code><a href="answer_as_list.html">answer_as_list</a>()</code>,
<code><a href="answer_as_named_list.html">answer_as_named_list</a>()</code>,
<code><a href="answer_as_regex.html">answer_as_regex</a>()</code>,
<code><a href="answer_by_chain_of_thought.html">answer_by_chain_of_thought</a>()</code>,
<code><a href="answer_by_react.html">answer_by_react</a>()</code>,
<code><a href="prompt_wrap.html">prompt_wrap</a>()</code>,
<code><a href="quit_if.html">quit_if</a>()</code>,
<code><a href="set_system_prompt.html">set_system_prompt</a>()</code></p>
<p>Other answer_as_prompt_wraps:
<code><a href="answer_as_boolean.html">answer_as_boolean</a>()</code>,
<code><a href="answer_as_code.html">answer_as_code</a>()</code>,
<code><a href="answer_as_integer.html">answer_as_integer</a>()</code>,
<code><a href="answer_as_list.html">answer_as_list</a>()</code>,
<code><a href="answer_as_named_list.html">answer_as_named_list</a>()</code>,
<code><a href="answer_as_regex.html">answer_as_regex</a>()</code></p>
<p>Other answer_as_json:
<code><a href="generate_json_example_from_schema.html">generate_json_example_from_schema</a>()</code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="va">base_prompt</span> <span class="op">&lt;-</span> <span class="st">"How can I solve 8x + 7 = -23?"</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">#### Enforcing JSON without a schema: ####</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span>  <span class="co"># Text-based (works with any model and all providers):</span></span></span>
<span class="r-in"><span>  <span class="co">#   Adds request for JSON object to the prompt</span></span></span>
<span class="r-in"><span>  <span class="co">#   Extracts the JSON objects from the LLM response</span></span></span>
<span class="r-in"><span>  <span class="va">json1</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co"># Ollama-type without schema</span></span></span>
<span class="r-in"><span>  <span class="co">#   Sets Ollama's 'format' parameter to 'json', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   Adds request to prompt for a JSON object, as is recommended in documentation</span></span></span>
<span class="r-in"><span>  <span class="va">json2</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"ollama"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co"># OpenAI-type without schema</span></span></span>
<span class="r-in"><span>  <span class="co">#   Sets OpenAI's 'response_format' parameter to 'json_object', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   Adds request to prompt for a JSON object, as is required by the API</span></span></span>
<span class="r-in"><span>  <span class="va">json3</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"openai"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">#### Enforcing JSON with a schema: ####</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Make a list representing a JSON schema,</span></span></span>
<span class="r-in"><span><span class="co">#   which the LLM response must adhere to:</span></span></span>
<span class="r-in"><span><span class="va">json_schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  name <span class="op">=</span> <span class="st">"steps_to_solve"</span>, <span class="co"># Required for OpenAI API</span></span></span>
<span class="r-in"><span>  description <span class="op">=</span> <span class="cn">NULL</span>, <span class="co"># Optional for OpenAI API</span></span></span>
<span class="r-in"><span>  schema <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>    type <span class="op">=</span> <span class="st">"object"</span>,</span></span>
<span class="r-in"><span>    properties <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>      steps <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>        type <span class="op">=</span> <span class="st">"array"</span>,</span></span>
<span class="r-in"><span>        items <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>          type <span class="op">=</span> <span class="st">"object"</span>,</span></span>
<span class="r-in"><span>          properties <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>            explanation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"string"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>            output <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"string"</span><span class="op">)</span></span></span>
<span class="r-in"><span>          <span class="op">)</span>,</span></span>
<span class="r-in"><span>          required <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"explanation"</span>, <span class="st">"output"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>          additionalProperties <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span>        <span class="op">)</span></span></span>
<span class="r-in"><span>      <span class="op">)</span>,</span></span>
<span class="r-in"><span>      final_answer <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"string"</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="op">)</span>,</span></span>
<span class="r-in"><span>    required <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"steps"</span>, <span class="st">"final_answer"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>    additionalProperties <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span>  <span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># 'strict' parameter is set as argument 'answer_as_json()'</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Generate example R object based on schema:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="generate_json_example_from_schema.html">generate_json_example_from_schema</a></span><span class="op">(</span><span class="va">json_schema</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps[[1]]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps[[1]]$explanation</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "..."</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps[[1]]$output</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "..."</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $final_answer</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "..."</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span>  <span class="co"># Text-based with schema</span></span></span>
<span class="r-in"><span>  <span class="co">#   Adds request for JSON &amp; example based on schema to the prompt;</span></span></span>
<span class="r-in"><span>  <span class="co">#   extracts JSON and validates against the schema with</span></span></span>
<span class="r-in"><span>  <span class="co">#   the 'jsonvalidate' package</span></span></span>
<span class="r-in"><span>  <span class="va">json4</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>schema <span class="op">=</span> <span class="va">json_schema</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co"># Ollama with schema</span></span></span>
<span class="r-in"><span>  <span class="co">#   Sets Ollama's 'format' parameter to 'json', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   Adds example based on schema to the prompt;</span></span></span>
<span class="r-in"><span>  <span class="co">#   extracts JSON and validates against the schema with</span></span></span>
<span class="r-in"><span>  <span class="co">#   the 'jsonvalidate' package</span></span></span>
<span class="r-in"><span>  <span class="va">json5</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"ollama"</span>, schema <span class="op">=</span> <span class="va">json_schema</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co"># OpenAI with schema</span></span></span>
<span class="r-in"><span>  <span class="co">#   Sets OpenAI's 'response_format' parameter to 'json_schema',</span></span></span>
<span class="r-in"><span>  <span class="co">#   and adds the json_schema to the API request;</span></span></span>
<span class="r-in"><span>  <span class="co">#   as such, the API will natively enforce the schema</span></span></span>
<span class="r-in"><span>  <span class="va">json6</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"openai"</span>, schema <span class="op">=</span> <span class="va">json_schema</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

