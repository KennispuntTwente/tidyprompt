<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Make LLM answer as JSON (with optional schema) — answer_as_json • tidyprompt</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Make LLM answer as JSON (with optional schema) — answer_as_json"><meta name="description" content="This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
The function can work with all models and providers through text-based
handling, but also supports native settings for the OpenAI and Ollama
API types. (See argument 'type'.) This means that it is possible to easily
switch between providers with different levels of JSON support,
while ensuring the results will be in the correct format."><meta property="og:description" content="This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
The function can work with all models and providers through text-based
handling, but also supports native settings for the OpenAI and Ollama
API types. (See argument 'type'.) This means that it is possible to easily
switch between providers with different levels of JSON support,
while ensuring the results will be in the correct format."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.1.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/creating_prompt_wraps.html">Creating prompt wraps</a></li>
    <li><a class="dropdown-item" href="../articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="../articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjarkvandemerwe/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Make LLM answer as JSON (with optional schema)</h1>
      <small class="dont-index">Source: <a href="https://github.com/tjarkvandemerwe/tidyprompt/blob/main/R/answer_as_json.R" class="external-link"><code>R/answer_as_json.R</code></a></small>
      <div class="d-none name"><code>answer_as_json.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.</p>
<p>The function can work with all models and providers through text-based
handling, but also supports native settings for the OpenAI and Ollama
API types. (See argument 'type'.) This means that it is possible to easily
switch between providers with different levels of JSON support,
while ensuring the results will be in the correct format.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">answer_as_json</span><span class="op">(</span></span>
<span>  <span class="va">prompt</span>,</span>
<span>  schema <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  schema_strict <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  schema_in_prompt_as <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"example"</span>, <span class="st">"schema"</span><span class="op">)</span>,</span>
<span>  type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"text-based"</span>, <span class="st">"auto"</span>, <span class="st">"openai"</span>, <span class="st">"ollama"</span>, <span class="st">"openai_oo"</span>, <span class="st">"ollama_oo"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-prompt">prompt<a class="anchor" aria-label="anchor" href="#arg-prompt"></a></dt>
<dd><p>A single string or a <code><a href="tidyprompt.html">tidyprompt()</a></code> object</p></dd>


<dt id="arg-schema">schema<a class="anchor" aria-label="anchor" href="#arg-schema"></a></dt>
<dd><p>A list which represents
a JSON schema that the response should match.See example and your API's
documentation for more information on defining JSON schemas. Note that the schema should be a
list (R object) representing a JSON schema, not a JSON string
(use <code><a href="https://jeroen.r-universe.dev/jsonlite/reference/fromJSON.html" class="external-link">jsonlite::fromJSON()</a></code> and <code><a href="https://jeroen.r-universe.dev/jsonlite/reference/fromJSON.html" class="external-link">jsonlite::toJSON()</a></code> to convert between the two)</p></dd>


<dt id="arg-schema-strict">schema_strict<a class="anchor" aria-label="anchor" href="#arg-schema-strict"></a></dt>
<dd><p>If TRUE, the provided schema will be strictly enforced.
This option is passed as part of the schema when using type  type
"openai" or "ollama", and when using the other types it is passed to
<code><a href="https://docs.ropensci.org/jsonvalidate/reference/json_validate.html" class="external-link">jsonvalidate::json_validate()</a></code></p></dd>


<dt id="arg-schema-in-prompt-as">schema_in_prompt_as<a class="anchor" aria-label="anchor" href="#arg-schema-in-prompt-as"></a></dt>
<dd><p>If providing a schema and
when using type "text-based", "openai_oo", or "ollama_oo", this argument specifies
how the schema should be included in the prompt:</p><ul><li><p>"example" (default): The schema will be included as an example JSON object
(tends to work best). <code><a href="r_json_schema_to_example.html">r_json_schema_to_example()</a></code> is used to generate the example object
from the schema</p></li>
<li><p>"schema": The schema will be included as a JSON schema</p></li>
</ul></dd>


<dt id="arg-type">type<a class="anchor" aria-label="anchor" href="#arg-type"></a></dt>
<dd><p>The way that JSON response should be enforced:</p><ul><li><p>"text-based": Instruction will be added to the prompt
asking for JSON; when a schema is provided, this will also be included
in the prompt (see argument 'schema_in_prompt_as'). JSON will be parsed
from the LLM response and, when a schema is provided, it will be validated
against the schema with <code><a href="https://docs.ropensci.org/jsonvalidate/reference/json_validate.html" class="external-link">jsonvalidate::json_validate()</a></code>. Feedback is sent to the
LLM when the response is not valid. This option always works, but may in some
cases may be less powerful than the other native JSON options</p></li>
<li><p>"auto": Automatically determine the type based on 'llm_provider$api_type'.
This does not consider model compatibility and could lead to errors; set 'type'
manually if errors occur; use 'text-based' if unsure</p></li>
<li><p>"openai" and "ollama": The response format will be set via the relevant API parameters,
making the API enforce a valid JSON response. If a schema is provided,
it will also be included in the API parameters and also be enforced by the API.
When no schema is provided, a request for JSON is added to the prompt (as required
by the APIs). Note that these JSON options may not be available for all models
of your provider; consult their documentation for more information.
If you are unsure or encounter errors, use "text-based"</p></li>
<li><p>"openai_oo" and "ollama_oo": Similar to "openai" and "ollama", but if a
schema is provided it is not included in the API parameters. Schema validation
will be done in R with <code><a href="https://docs.ropensci.org/jsonvalidate/reference/json_validate.html" class="external-link">jsonvalidate::json_validate()</a></code>. This can be useful if
you want to use the API's JSON support, but their schema support is limited</p></li>
</ul><p>Note that the "openai" and "ollama" types may also work for other APIs with a similar structure</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A <code><a href="tidyprompt.html">tidyprompt()</a></code> with an added <code><a href="prompt_wrap.html">prompt_wrap()</a></code> which will ensure
that the LLM response is a valid JSON object</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>Other pre_built_prompt_wraps:
<code><a href="add_text.html">add_text</a>()</code>,
<code><a href="answer_as_boolean.html">answer_as_boolean</a>()</code>,
<code><a href="answer_as_integer.html">answer_as_integer</a>()</code>,
<code><a href="answer_as_list.html">answer_as_list</a>()</code>,
<code><a href="answer_as_named_list.html">answer_as_named_list</a>()</code>,
<code><a href="answer_as_regex_match.html">answer_as_regex_match</a>()</code>,
<code><a href="answer_as_text.html">answer_as_text</a>()</code>,
<code><a href="answer_by_chain_of_thought.html">answer_by_chain_of_thought</a>()</code>,
<code><a href="answer_by_react.html">answer_by_react</a>()</code>,
<code><a href="answer_using_r.html">answer_using_r</a>()</code>,
<code><a href="answer_using_sql.html">answer_using_sql</a>()</code>,
<code><a href="answer_using_tools.html">answer_using_tools</a>()</code>,
<code><a href="prompt_wrap.html">prompt_wrap</a>()</code>,
<code><a href="quit_if.html">quit_if</a>()</code>,
<code><a href="set_system_prompt.html">set_system_prompt</a>()</code></p>
<p>Other answer_as_prompt_wraps:
<code><a href="answer_as_boolean.html">answer_as_boolean</a>()</code>,
<code><a href="answer_as_integer.html">answer_as_integer</a>()</code>,
<code><a href="answer_as_list.html">answer_as_list</a>()</code>,
<code><a href="answer_as_named_list.html">answer_as_named_list</a>()</code>,
<code><a href="answer_as_regex_match.html">answer_as_regex_match</a>()</code>,
<code><a href="answer_as_text.html">answer_as_text</a>()</code></p>
<p>Other json:
<code><a href="r_json_schema_to_example.html">r_json_schema_to_example</a>()</code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="va">base_prompt</span> <span class="op">&lt;-</span> <span class="st">"How can I solve 8x + 7 = -23?"</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># This example will show how to enforce JSON format in the response,</span></span></span>
<span class="r-in"><span><span class="co">#   with and without a schema, using the 'answer_as_json()' prompt wrap.</span></span></span>
<span class="r-in"><span><span class="co"># If you use type = 'auto', the function will automatically detect the</span></span></span>
<span class="r-in"><span><span class="co">#   best way to enforce JSON based on the LLM provider you are using.</span></span></span>
<span class="r-in"><span><span class="co"># Note that the default type is 'text-based', which will work for any provider/model</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">#### Enforcing JSON without a schema: ####</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span>  <span class="co">## Text-based (works for any provider/model):</span></span></span>
<span class="r-in"><span>  <span class="co">#   Adds request to prompt for a JSON object</span></span></span>
<span class="r-in"><span>  <span class="co">#   Extracts JSON from textual response (feedback for retry if no JSON received)</span></span></span>
<span class="r-in"><span>  <span class="co">#   Parses JSON to R object</span></span></span>
<span class="r-in"><span>  <span class="va">json_1</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (llama3.1:8b): ---</span></span></span>
<span class="r-in"><span>  <span class="co"># How can I solve 8x + 7 = -23?</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your must format your response as a JSON object.</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co"># Here is the solution to the equation formatted as a JSON object:</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># ```</span></span></span>
<span class="r-in"><span>  <span class="co"># {</span></span></span>
<span class="r-in"><span>  <span class="co">#   "equation": "8x + 7 = -23",</span></span></span>
<span class="r-in"><span>  <span class="co">#   "steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "step": "Subtract 7 from both sides of the equation",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "expression": "-23 - 7"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "step": "Simplify the expression on the left side",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "result": "-30"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "step": "Divide both sides by -8 to solve for x",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "expression": "-30 / -8"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "step": "Simplify the expression on the right side",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "result": "3.75"</span></span></span>
<span class="r-in"><span>  <span class="co">#     }</span></span></span>
<span class="r-in"><span>  <span class="co">#   ],</span></span></span>
<span class="r-in"><span>  <span class="co">#   "solution": {</span></span></span>
<span class="r-in"><span>  <span class="co">#     "x": 3.75</span></span></span>
<span class="r-in"><span>  <span class="co">#   }</span></span></span>
<span class="r-in"><span>  <span class="co"># }</span></span></span>
<span class="r-in"><span>  <span class="co"># ```</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co">## Ollama:</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Sets 'format' parameter to 'json', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds request to prompt for a JSON object, as is recommended by the docs</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Parses JSON to R object</span></span></span>
<span class="r-in"><span>  <span class="va">json_2</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"auto"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (llama3.1:8b): ---</span></span></span>
<span class="r-in"><span>  <span class="co"># How can I solve 8x + 7 = -23?</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your must format your response as a JSON object.</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co"># {"steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#   "Subtract 7 from both sides to get 8x = -30",</span></span></span>
<span class="r-in"><span>  <span class="co">#   "Simplify the right side of the equation to get 8x = -30",</span></span></span>
<span class="r-in"><span>  <span class="co">#   "Divide both sides by 8 to solve for x, resulting in x = -30/8",</span></span></span>
<span class="r-in"><span>  <span class="co">#   "Simplify the fraction to find the value of x"</span></span></span>
<span class="r-in"><span>  <span class="co"># ],</span></span></span>
<span class="r-in"><span>  <span class="co"># "value_of_x": "-3.75"}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co">## OpenAI-type API without schema:</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Sets 'response_format' parameter to 'json_object', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds request to prompt for a JSON object, as is required by the API</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Parses JSON to R object</span></span></span>
<span class="r-in"><span>  <span class="va">json_3</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>type <span class="op">=</span> <span class="st">"auto"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (gpt-4o-mini): ---</span></span></span>
<span class="r-in"><span>  <span class="co"># How can I solve 8x + 7 = -23?</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your must format your response as a JSON object.</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co"># {</span></span></span>
<span class="r-in"><span>  <span class="co">#   "solution_steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "step": 1,</span></span></span>
<span class="r-in"><span>  <span class="co">#       "operation": "Subtract 7 from both sides",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "equation": "8x + 7 - 7 = -23 - 7",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "result": "8x = -30"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "step": 2,</span></span></span>
<span class="r-in"><span>  <span class="co">#       "operation": "Divide both sides by 8",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "equation": "8x / 8 = -30 / 8",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "result": "x = -3.75"</span></span></span>
<span class="r-in"><span>  <span class="co">#     }</span></span></span>
<span class="r-in"><span>  <span class="co">#   ],</span></span></span>
<span class="r-in"><span>  <span class="co">#   "solution": {</span></span></span>
<span class="r-in"><span>  <span class="co">#     "x": -3.75</span></span></span>
<span class="r-in"><span>  <span class="co">#   }</span></span></span>
<span class="r-in"><span>  <span class="co"># }</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">#### Enforcing JSON with a schema: ####</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Make a list representing a JSON schema,</span></span></span>
<span class="r-in"><span><span class="co">#   which the LLM response must adhere to:</span></span></span>
<span class="r-in"><span><span class="va">json_schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  name <span class="op">=</span> <span class="st">"steps_to_solve"</span>, <span class="co"># Required for OpenAI API</span></span></span>
<span class="r-in"><span>  description <span class="op">=</span> <span class="cn">NULL</span>, <span class="co"># Optional for OpenAI API</span></span></span>
<span class="r-in"><span>  schema <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>    type <span class="op">=</span> <span class="st">"object"</span>,</span></span>
<span class="r-in"><span>    properties <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>      steps <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>        type <span class="op">=</span> <span class="st">"array"</span>,</span></span>
<span class="r-in"><span>        items <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>          type <span class="op">=</span> <span class="st">"object"</span>,</span></span>
<span class="r-in"><span>          properties <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>            explanation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"string"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>            output <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"string"</span><span class="op">)</span></span></span>
<span class="r-in"><span>          <span class="op">)</span>,</span></span>
<span class="r-in"><span>          required <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"explanation"</span>, <span class="st">"output"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>          additionalProperties <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span>        <span class="op">)</span></span></span>
<span class="r-in"><span>      <span class="op">)</span>,</span></span>
<span class="r-in"><span>      final_answer <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"string"</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="op">)</span>,</span></span>
<span class="r-in"><span>    required <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"steps"</span>, <span class="st">"final_answer"</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>    additionalProperties <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span>  <span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># 'strict' parameter is set as argument 'answer_as_json()'</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co"># Note: when you are not using an OpenAI API, you can also pass just the</span></span></span>
<span class="r-in"><span><span class="co">#   internal 'schema' list object to 'answer_as_json()' instead of the full</span></span></span>
<span class="r-in"><span><span class="co">#   'json_schema' list object</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Generate example R object based on schema:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="r_json_schema_to_example.html">r_json_schema_to_example</a></span><span class="op">(</span><span class="va">json_schema</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps[[1]]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps[[1]]$explanation</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "..."</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $steps[[1]]$output</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "..."</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $final_answer</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "..."</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span>  <span class="co">## Text-based with schema (works for any provider/model):</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds request to prompt for a JSON object</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds schema to prompt</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Extracts JSON from textual response (feedback for retry if no JSON received)</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Parses JSON to R object</span></span></span>
<span class="r-in"><span>  <span class="va">json_4</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span>schema <span class="op">=</span> <span class="va">json_schema</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (llama3.1:8b): ---</span></span></span>
<span class="r-in"><span>  <span class="co"># How can I solve 8x + 7 = -23?</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your must format your response as a JSON object.</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your JSON object should match this example JSON object:</span></span></span>
<span class="r-in"><span>  <span class="co">#   {</span></span></span>
<span class="r-in"><span>  <span class="co">#     "steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#       {</span></span></span>
<span class="r-in"><span>  <span class="co">#         "explanation": "...",</span></span></span>
<span class="r-in"><span>  <span class="co">#         "output": "..."</span></span></span>
<span class="r-in"><span>  <span class="co">#       }</span></span></span>
<span class="r-in"><span>  <span class="co">#     ],</span></span></span>
<span class="r-in"><span>  <span class="co">#     "final_answer": "..."</span></span></span>
<span class="r-in"><span>  <span class="co">#   }</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co"># Here is the solution to the equation:</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># ```</span></span></span>
<span class="r-in"><span>  <span class="co"># {</span></span></span>
<span class="r-in"><span>  <span class="co">#   "steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "First, we want to isolate the term with 'x' by</span></span></span>
<span class="r-in"><span>  <span class="co">#       subtracting 7 from both sides of the equation.",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "8x + 7 - 7 = -23 - 7"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "This simplifies to: 8x = -30",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "8x = -30"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "Next, we want to get rid of the coefficient '8' by</span></span></span>
<span class="r-in"><span>  <span class="co">#       dividing both sides of the equation by 8.",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "(8x) / 8 = (-30) / 8"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "This simplifies to: x = -3.75",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "x = -3.75"</span></span></span>
<span class="r-in"><span>  <span class="co">#     }</span></span></span>
<span class="r-in"><span>  <span class="co">#   ],</span></span></span>
<span class="r-in"><span>  <span class="co">#   "final_answer": "-3.75"</span></span></span>
<span class="r-in"><span>  <span class="co"># }</span></span></span>
<span class="r-in"><span>  <span class="co"># ```</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co">## Ollama with schema:</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Sets 'format' parameter to 'json', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds request to prompt for a JSON object, as is recommended by the docs</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds schema to prompt</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)</span></span></span>
<span class="r-in"><span>  <span class="va">json_5</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span><span class="va">json_schema</span>, type <span class="op">=</span> <span class="st">"auto"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (llama3.1:8b): ---</span></span></span>
<span class="r-in"><span>  <span class="co"># How can I solve 8x + 7 = -23?</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your must format your response as a JSON object.</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># Your JSON object should match this example JSON object:</span></span></span>
<span class="r-in"><span>  <span class="co"># {</span></span></span>
<span class="r-in"><span>  <span class="co">#   "steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "...",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "..."</span></span></span>
<span class="r-in"><span>  <span class="co">#     }</span></span></span>
<span class="r-in"><span>  <span class="co">#   ],</span></span></span>
<span class="r-in"><span>  <span class="co">#   "final_answer": "..."</span></span></span>
<span class="r-in"><span>  <span class="co"># }</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co"># {</span></span></span>
<span class="r-in"><span>  <span class="co">#   "steps": [</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "First, subtract 7 from both sides of the equation to</span></span></span>
<span class="r-in"><span>  <span class="co">#       isolate the term with x.",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "8x = -23 - 7"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "Simplify the right-hand side of the equation.",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "8x = -30"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "Next, divide both sides of the equation by 8 to solve for x.",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "x = -30 / 8"</span></span></span>
<span class="r-in"><span>  <span class="co">#     },</span></span></span>
<span class="r-in"><span>  <span class="co">#     {</span></span></span>
<span class="r-in"><span>  <span class="co">#       "explanation": "Simplify the right-hand side of the equation.",</span></span></span>
<span class="r-in"><span>  <span class="co">#       "output": "x = -3.75"</span></span></span>
<span class="r-in"><span>  <span class="co">#     }</span></span></span>
<span class="r-in"><span>  <span class="co">#   ],</span></span></span>
<span class="r-in"><span>  <span class="co">#   "final_answer": "-3.75"</span></span></span>
<span class="r-in"><span>  <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="co">## OpenAI with schema:</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Sets 'response_format' parameter to 'json_object', enforcing JSON</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Adds json_schema to the API request, API enforces JSON adhering schema</span></span></span>
<span class="r-in"><span>  <span class="co">#   - Parses JSON to R object</span></span></span>
<span class="r-in"><span>  <span class="va">json_6</span> <span class="op">&lt;-</span> <span class="va">base_prompt</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">answer_as_json</span><span class="op">(</span><span class="va">json_schema</span>, type <span class="op">=</span> <span class="st">"auto"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="fu"><a href="llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (gpt-4o-mini): ---</span></span></span>
<span class="r-in"><span>  <span class="co"># How can I solve 8x + 7 = -23?</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co"># {"steps":[</span></span></span>
<span class="r-in"><span>  <span class="co"># {"explanation":"Start with the original equation.",</span></span></span>
<span class="r-in"><span>  <span class="co"># "output":"8x + 7 = -23"},</span></span></span>
<span class="r-in"><span>  <span class="co"># {"explanation":"Subtract 7 from both sides to isolate the term with x.",</span></span></span>
<span class="r-in"><span>  <span class="co"># "output":"8x + 7 - 7 = -23 - 7"},</span></span></span>
<span class="r-in"><span>  <span class="co"># {"explanation":"Simplify the left side and the right side of the equation.",</span></span></span>
<span class="r-in"><span>  <span class="co"># "output":"8x = -30"},</span></span></span>
<span class="r-in"><span>  <span class="co"># {"explanation":"Now, divide both sides by 8 to solve for x.",</span></span></span>
<span class="r-in"><span>  <span class="co"># "output":"x = -30 / 8"},</span></span></span>
<span class="r-in"><span>  <span class="co"># {"explanation":"Simplify the fraction by dividing both the numerator and the</span></span></span>
<span class="r-in"><span>  <span class="co"># denominator by 2.",</span></span></span>
<span class="r-in"><span>  <span class="co"># "output":"x = -15 / 4"}</span></span></span>
<span class="r-in"><span>  <span class="co"># ], "final_answer":"x = -15/4"}</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

