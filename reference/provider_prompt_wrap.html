<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Create a provider-level prompt wrap — provider_prompt_wrap • tidyprompt</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Create a provider-level prompt wrap — provider_prompt_wrap"><meta name="description" content="
Build a provider-specific prompt wrap, to store on an llm_provider object
(with $add_prompt_wrap()). These prompt wraps can be applied before or
after any prompt-specific prompt wraps. In this way, you can ensure that
certain prompt wraps are always applied when using a specific LLM provider."><meta property="og:description" content="
Build a provider-specific prompt wrap, to store on an llm_provider object
(with $add_prompt_wrap()). These prompt wraps can be applied before or
after any prompt-specific prompt wraps. In this way, you can ensure that
certain prompt wraps are always applied when using a specific LLM provider."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/creating_prompt_wraps.html">Creating prompt wraps</a></li>
    <li><a class="dropdown-item" href="../articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="../articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/KennispuntTwente/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Create a provider-level prompt wrap</h1>
      <small class="dont-index">Source: <a href="https://github.com/KennispuntTwente/tidyprompt/blob/main/R/prompt_wrap.R" class="external-link"><code>R/prompt_wrap.R</code></a></small>
      <div class="d-none name"><code>provider_prompt_wrap.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental" class="external-link"><img src="figures/lifecycle-experimental.svg" alt="[Experimental]"></a>
Build a provider-specific prompt wrap, to store on an <a href="llm_provider-class.html">llm_provider</a> object
(with <code>$add_prompt_wrap()</code>). These prompt wraps can be applied before or
after any prompt-specific prompt wraps. In this way, you can ensure that
certain prompt wraps are always applied when using a specific LLM provider.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">provider_prompt_wrap</span><span class="op">(</span></span>
<span>  modify_fn <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  extraction_fn <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  validation_fn <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  handler_fn <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  parameter_fn <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"unspecified"</span>, <span class="st">"mode"</span>, <span class="st">"tool"</span>, <span class="st">"break"</span>, <span class="st">"check"</span><span class="op">)</span>,</span>
<span>  name <span class="op">=</span> <span class="cn">NULL</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-modify-fn">modify_fn<a class="anchor" aria-label="anchor" href="#arg-modify-fn"></a></dt>
<dd><p>A function that takes the previous prompt text (as
first argument) and returns the new prompt text</p></dd>


<dt id="arg-extraction-fn">extraction_fn<a class="anchor" aria-label="anchor" href="#arg-extraction-fn"></a></dt>
<dd><p>A function that takes the LLM response (as first argument)
and attempts to extract a value from it.
Upon succesful extraction, the function should return the extracted value.
If the extraction fails, the function should return a <code><a href="llm_feedback.html">llm_feedback()</a></code> message
to initiate a retry.
A <code><a href="llm_break.html">llm_break()</a></code> can be returned to break the extraction and validation loop,
ending <code><a href="send_prompt.html">send_prompt()</a></code></p></dd>


<dt id="arg-validation-fn">validation_fn<a class="anchor" aria-label="anchor" href="#arg-validation-fn"></a></dt>
<dd><p>A function that takes the (extracted) LLM response
(as first argument) and attempts to validate it.
Upon succesful validation, the function should return TRUE. If the validation
fails, the function should return a <code><a href="llm_feedback.html">llm_feedback()</a></code> message to initiate a retry.
A <code><a href="llm_break.html">llm_break()</a></code> can be returned to break the extraction and validation loop,
ending <code><a href="send_prompt.html">send_prompt()</a></code></p></dd>


<dt id="arg-handler-fn">handler_fn<a class="anchor" aria-label="anchor" href="#arg-handler-fn"></a></dt>
<dd><p>A function that takes a 'completion' object (a result
of a request to a LLM, as returned by <code>$complete_chat()</code> of a <a href="llm_provider-class.html">llm_provider</a>
object) as first argument and the <a href="llm_provider-class.html">llm_provider</a> object as second argument.
The function should return a (modified or identical) completion object.
This can be used for advanced side effects, like logging, or native tool calling,
or keeping track of token usage. See <a href="llm_provider-class.html">llm_provider</a> for more information;
handler_fn is attached to the <a href="llm_provider-class.html">llm_provider</a> object that is being used.
When using an <code><a href="llm_provider_ellmer.html">llm_provider_ellmer()</a></code>, the up-to-date ellmer_chat is synced
onto the provider before handlers run. This allows handlers to access,
for instance, the current cost of the conversation, and, for instance,
to stop the conversation if a certain budget is exceeded.
For example usage, see source code of <code><a href="answer_using_tools.html">answer_using_tools()</a></code></p></dd>


<dt id="arg-parameter-fn">parameter_fn<a class="anchor" aria-label="anchor" href="#arg-parameter-fn"></a></dt>
<dd><p>A function that takes the <a href="llm_provider-class.html">llm_provider</a> object
which is being used with <code><a href="send_prompt.html">send_prompt()</a></code> and returns a named list of parameters
to be set in the <a href="llm_provider-class.html">llm_provider</a> object via its <code>$set_parameters()</code> method.
This can be used to configure specific parameters of the <a href="llm_provider-class.html">llm_provider</a>
object when evaluating the prompt.
For example, <code><a href="answer_as_json.html">answer_as_json()</a></code> may set different parameters for different APIs
related to JSON output.
This function is typically only used with advanced prompt wraps that require
specific settings in the <a href="llm_provider-class.html">llm_provider</a> object</p></dd>


<dt id="arg-type">type<a class="anchor" aria-label="anchor" href="#arg-type"></a></dt>
<dd><p>The type of prompt wrap. Must be one of:</p><ul><li><p>"unspecified": The default type, typically used for prompt wraps
which request a specific format of the LLM response, like <code><a href="answer_as_integer.html">answer_as_integer()</a></code></p></li>
<li><p>"mode": For prompt wraps that change how the LLM should answer the prompt,
like <code><a href="answer_by_chain_of_thought.html">answer_by_chain_of_thought()</a></code> or <code><a href="answer_by_react.html">answer_by_react()</a></code></p></li>
<li><p>"tool": For prompt wraps that enable the LLM to use tools, like <code><a href="answer_using_tools.html">answer_using_tools()</a></code>
or <code><a href="answer_using_r.html">answer_using_r()</a></code> when 'output_as_tool' = TRUE</p></li>
<li><p>"break": For prompt wraps that may break the extraction and validation loop,
like <code><a href="quit_if.html">quit_if()</a></code>. These are applied before type "unspecified" as they may
instruct the LLM to not answer the prompt in the manner specified by those
prompt wraps</p></li>
<li><p>"check": For prompt wraps that apply a last check to the final answer,
after all other prompt wraps have been evaluated.
These prompt wraps may only contain a validation function, and are applied
after all other prompt wraps have been evaluated. These prompt wraps are
even applied after an earlier prompt wrap has broken the extraction and validation loop
with <code><a href="llm_break.html">llm_break()</a></code></p></li>
</ul><p>Types are used to determine the order in which prompt wraps are applied.
When constructing the prompt text, prompt wraps are applied to the base prompt
in the following order: 'check', 'unspecified', 'break', 'mode', 'tool'.
When evaluating the LLM response and applying extraction and validation functions,
prompt wraps are applied in the reverse order: 'tool', 'mode', 'break',
'unspecified', 'check'.
Order among the same type is preserved in the order they were added to the prompt.</p></dd>


<dt id="arg-name">name<a class="anchor" aria-label="anchor" href="#arg-name"></a></dt>
<dd><p>An optional name for the prompt wrap.
This can be used to identify the prompt wrap in the <a href="tidyprompt-class.html">tidyprompt</a> object</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A <code>provider_prompt_wrap</code> object, to be stored on an <a href="llm_provider-class.html">llm_provider</a> object</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="va">ollama</span> <span class="op">&lt;-</span> <span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Add a "short answer" mode (provider-level post prompt wrap)</span></span></span>
<span class="r-in"><span><span class="va">ollama</span><span class="op">$</span><span class="fu">add_prompt_wrap</span><span class="op">(</span></span></span>
<span class="r-in"><span>  <span class="fu">provider_prompt_wrap</span><span class="op">(</span></span></span>
<span class="r-in"><span>    modify_fn <span class="op">=</span> \<span class="op">(</span><span class="va">txt</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>      <span class="va">txt</span>,</span></span>
<span class="r-in"><span>      <span class="st">"\n\nPlease answer concisely (&lt; 2 sentences)."</span></span></span>
<span class="r-in"><span>    <span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="op">)</span>,</span></span>
<span class="r-in"><span>  position <span class="op">=</span> <span class="st">"post"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Use as usual: wraps are applied automatically</span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="st">"What's a vignette in R?"</span> <span class="op">|&gt;</span> <span class="fu"><a href="send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

