<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Getting started • tidyprompt</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Getting started">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/creating_prompt_wraps.html">Creating prompt wraps</a></li>
    <li><a class="dropdown-item" href="../articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="../articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/KennispuntTwente/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Getting started</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/KennispuntTwente/tidyprompt/blob/0.1.0/vignettes/getting_started.Rmd" class="external-link"><code>vignettes/getting_started.Rmd</code></a></small>
      <div class="d-none name"><code>getting_started.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/KennispuntTwente/tidyprompt" class="external-link">tidyprompt</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="setup-an-llm-provider">Setup an LLM provider<a class="anchor" aria-label="anchor" href="#setup-an-llm-provider"></a>
</h2>
<p>‘tidyprompt’ can be used with any LLM provider capable of completing
a chat.</p>
<p>At the moment, ‘tidyprompt’ includes pre-built functions to connect
with various LLM providers, such as Ollama, OpenAI, OpenRouter, Mistral,
Groq, XAI (Grok), and Google Gemini.</p>
<p>An (experimental) function is also included with which users can user
an <code><a href="https://ellmer.tidyverse.org/reference/chat-any.html" class="external-link">ellmer::chat()</a></code> object to build an LLM provider; this
allows you to use any LLM provider you can configure with the ‘ellmer’ R
package (including its respective configuration and features).</p>
<p>Furthermore, with the <code>llm_provider-class</code>, you can easily
write a hook for any other LLM provider. You could make API calls using
the ‘httr2’ package or use another R package that already has a hook for
the LLM provider you want to use. If your API of choice follows the
structure of the OpenAI API, you can call
<code><a href="../reference/llm_provider_openai.html">llm_provider_openai()</a></code> and change the relevant parameters
(like the URL and the API key).</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ollama running on local PC</span></span>
<span><span class="va">ollama</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"llama3.1:8b"</span><span class="op">)</span>,</span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># OpenAI API</span></span>
<span><span class="va">openai</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Various providers via OpenRouter (e.g., Anthropic)</span></span>
<span><span class="va">openrouter</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_openrouter.html">llm_provider_openrouter</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"anthropic/claude-3.5-sonnet"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ... functions also included for Mistral, Groq, XAI (Grok), and Google Gemini</span></span>
<span></span>
<span><span class="co"># Alternatively, you can use an `ellmer:chat()` object (e.g., `ellmer::chat_openai()`)</span></span>
<span><span class="co"># (See also: https://ellmer.tidyverse.org/index.html)</span></span>
<span><span class="va">ellmer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_ellmer.html">llm_provider_ellmer</a></span><span class="op">(</span><span class="fu">ellmer</span><span class="fu">::</span><span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html" class="external-link">chat_openai</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ... or easily create your own hook for any other LLM provider;</span></span>
<span><span class="co">#   see ?`llm_provider-class` for more information; also take a look at the source code of</span></span>
<span><span class="co">#   `llm_provider_ollama()` and `llm_provider_openai()`. For APIs that follow the structure</span></span>
<span><span class="co">#   of the OpenAI API for chat completion, you can use `llm_provider_openai()`</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="basic-prompting">Basic prompting<a class="anchor" aria-label="anchor" href="#basic-prompting"></a>
</h2>
<p>A simple string serves as the base for a prompt.</p>
<p>By adding prompt wraps, you can influence various aspects of how the
LLM handles the prompt, while verifying that the output is structured
and valid (including retries with feedback to the LLM if it is not).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi there!</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; How's your day going so far? Is there something I can help you with or would you like to chat?</span></span>
<span><span class="co">#&gt; [1] "How's your day going so far? Is there something I can help you with or would you like to chat?"</span></span></code></pre></div>
<p><code><a href="../reference/add_text.html">add_text()</a></code> is a simple example of a prompt wrap. It
simply adds some text at the end of the base prompt.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is a large language model? Explain in 10 words."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi there!</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; What is a large language model? Explain in 10 words.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Advanced computer program trained on vast amounts of written data.</span></span>
<span><span class="co">#&gt; [1] "Advanced computer program trained on vast amounts of written data."</span></span></code></pre></div>
<p>You can also construct the final prompt text, without sending it to
an LLM provider.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is a large language model? Explain in 10 words."</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;tidyprompt&gt;</span></span>
<span><span class="co">#&gt; The base prompt is modified by a prompt wrap, resulting in:</span></span>
<span><span class="co">#&gt; &gt; Hi there!</span></span>
<span><span class="co">#&gt; &gt; </span></span>
<span><span class="co">#&gt; &gt; What is a large language model? Explain in 10 words. </span></span>
<span><span class="co">#&gt; Use 'x$base_prompt' to show the base prompt text.</span></span>
<span><span class="co">#&gt; Use 'x$construct_prompt_text()' to get the full prompt text.</span></span>
<span><span class="co">#&gt; Use 'get_prompt_wraps(x)' to show the prompt wraps.</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prompt-wraps">Prompt wraps<a class="anchor" aria-label="anchor" href="#prompt-wraps"></a>
</h2>
<p>This package contains three main families of pre-built prompt wraps
which affect how a LLM handles a prompt:</p>
<ul>
<li>
<code>answer_as</code>: specify the <strong>format</strong> of the
output (e.g., integer, list, json)</li>
<li>
<code>answer_by</code>: specify a <strong>reasoning mode</strong> to
reach the answer (e.g., chain-of-thought, ReAct)</li>
<li>
<code>answer_using</code>: give the LLM <strong>tools</strong> to
reach the answer (e.g., R functions, R code, SQL)</li>
</ul>
<p>Below, we will show examples of each type of prompt wrap.</p>
<div class="section level3">
<h3 id="answer_as-retrieving-output-in-a-specific-format">answer_as: Retrieving output in a specific format<a class="anchor" aria-label="anchor" href="#answer_as-retrieving-output-in-a-specific-format"></a>
</h3>
<p>Using prompt wraps, you can force the LLM to return the output in a
specific format. You can also extract the output to turn it from a
character into another data type.</p>
<p>For instance, <code><a href="../reference/answer_as_integer.html">answer_as_integer()</a></code> adds a prompt wrap
which forces the LLM to reply with an integer.</p>
<p>To achieve this, the prompt wrap will add some text to the base
prompt, asking the LLM to reply with an integer. However, the prompt
wrap does more: it also will attempt to extract and validate the integer
from the LLM’s response. If extraction or validation fails, feedback is
sent back to the LLM, after which the LLM can retry answering the
prompt. Because the extraction function turns the original character
response into a numeric value, the final output from
<code><a href="../reference/send_prompt.html">send_prompt()</a></code> will also be a numeric type.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 4</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
<p>Below is an example of a prompt which will initially fail, but will
succeed after <code><a href="../reference/llm_feedback.html">llm_feedback()</a></code> and a retry.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"Please write out your reply in words, use no numbers."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span>add_instruction_to_prompt <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Please write out your reply in words, use no numbers.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Four.</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 4</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
<p>‘tidyprompt’ offers various other ‘answer_as’ functions, such as
<code><a href="../reference/answer_as_boolean.html">answer_as_boolean()</a></code>, <code><a href="../reference/answer_as_regex_match.html">answer_as_regex_match()</a></code>,
<code><a href="../reference/answer_as_named_list.html">answer_as_named_list()</a></code>, <code><a href="../reference/answer_as_text.html">answer_as_text()</a></code> and
<code><a href="../reference/answer_as_json.html">answer_as_json()</a></code>.</p>
<div class="section level5">
<h5 id="json-output">JSON output<a class="anchor" aria-label="anchor" href="#json-output"></a>
</h5>
<p><code><a href="../reference/answer_as_json.html">answer_as_json()</a></code> may be especially powerful when your
LLM provider and model have native support for returning JSON objects
and adhering to JSON schemas (e.g., OpenAI, Ollama). Text-based handling
is however always possible, also for providers which do not natively
support such functions. This means that you can always switch between
providers while ensuring the results will be in the correct format.</p>
<p>It should also be noted that native JSON enforcement may also
restrict the model too much, so it is always good to test and see what
works best for your use case. In our experience, text-based handling is
often more flexible and robust, especially when combining multiple
prompt wraps. Note also that JSON schemas may not enforce all qualities
you are looking for in the output, so you may still need to add
additional prompt wraps to ensure the output is as desired.</p>
</div>
</div>
<div class="section level3">
<h3 id="answer_by-adding-a-reasoning-mode-to-the-llm">answer_by: Adding a reasoning mode to the LLM<a class="anchor" aria-label="anchor" href="#answer_by-adding-a-reasoning-mode-to-the-llm"></a>
</h3>
<p>Prompt wraps may also be used to add a reasoning mode to the LLM. It
is hypothesized that this could improve the LLM’s performance on more
complex tasks.</p>
<p>For instance, <code><a href="../reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought()</a></code> will add
chain of thought reasoning mode to the prompt evaluation by the LLM. The
function wraps the base prompt text within a request for the LLM to
reason step by step, asking it to provide the final answer within
‘FINISH[<final answer here>]’. An extraction function then ensures only
the final answer is returned.</final></p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You are given a user's prompt.</span></span>
<span><span class="co">#&gt; To answer the user's prompt, you need to think step by step to arrive at a final answer.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ----- START OF USER'S PROMPT -----</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; ----- END OF USER'S PROMPT -----</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; What are the steps you would take to answer the user's prompt?</span></span>
<span><span class="co">#&gt; Describe your thought process in the following format:</span></span>
<span><span class="co">#&gt;   &gt;&gt; step 1: &lt;step 1 description&gt;</span></span>
<span><span class="co">#&gt;   &gt;&gt; step 2: &lt;step 2 description&gt;</span></span>
<span><span class="co">#&gt;   (etc.)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; When you are done, you must type:</span></span>
<span><span class="co">#&gt;   FINISH[&lt;put here your final answer to the user's prompt&gt;]</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Make sure your final answer follows the logical conclusion of your thought process.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Here are my thought steps:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; &gt;&gt; step 1: Understand the question - The prompt asks for the result of a simple arithmetic operation, specifically adding 2 and 2.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; &gt;&gt; step 2: Recall basic math facts - I recall that the sum of two identical numbers is equal to twice the value of each number. In this case, both numbers are 2.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; &gt;&gt; step 3: Apply the math fact - Using the knowledge from step 2, I calculate the result by multiplying 2 (the number being added) by 2 (the other number), which gives 4.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; &gt;&gt; step 4: Confirm the answer - Before providing a final response, I confirm that my calculation is correct. Adding 2 and 2 indeed equals 4.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; FINISH[4]</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="answer_using-have-the-llm-work-with-tools-and-code">answer_using: Have the LLM work with tools and code<a class="anchor" aria-label="anchor" href="#answer_using-have-the-llm-work-with-tools-and-code"></a>
</h3>
<div class="section level4">
<h4 id="tools-function-calling">Tools (function-calling)<a class="anchor" aria-label="anchor" href="#tools-function-calling"></a>
</h4>
<p>With <code><a href="../reference/answer_using_tools.html">answer_using_tools()</a></code>, you can enable your LLM to
call R functions. This enables the LLM to autonomously retrieve
additional information or take other actions.</p>
<p><code><a href="../reference/answer_using_tools.html">answer_using_tools()</a></code> automatically extracts
documentation when it is available for base R functions and functions
from packages. Types are inferred from the default arguments of the
function. If you want to define a custom function and/or override the
default documentation, you can use <code><a href="../reference/tools_add_docs.html">tools_add_docs()</a></code>. See
example usage in the documentation of
<code><a href="../reference/answer_using_tools.html">answer_using_tools()</a></code>.</p>
<p><code><a href="../reference/answer_using_tools.html">answer_using_tools()</a></code> supports both text-based function
calling and native function calling (via API parameters, currently
implemented for OpenAI and Ollama API structures).</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What are the files in my current directory?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_using_tools.html">answer_using_tools</a></span><span class="op">(</span><span class="va">list.files</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; ! `answer_using_tools()`, `tools_docs_to_text()`:</span></span>
<span><span class="co">#&gt; * Argument 'pattern' has an unknown type. Defaulting to 'string'</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What are the files in my current directory?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; If you need more information, you can call functions to help you.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; To call a function, output a JSON object with the following format:</span></span>
<span><span class="co">#&gt;   {</span></span>
<span><span class="co">#&gt;     "function": "&lt;function name&gt;",</span></span>
<span><span class="co">#&gt;     "arguments": {</span></span>
<span><span class="co">#&gt;       "&lt;argument_name&gt;": &lt;argument_value&gt;,</span></span>
<span><span class="co">#&gt;       # ...</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt;   (Note: you may not provide function calls as function arguments.)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The following functions are available:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   function name: list.files</span></span>
<span><span class="co">#&gt;   description: List the Files in a Directory/Folder: These functions produce a character vector of the names of files</span></span>
<span><span class="co">#&gt; or directories in the named directory.</span></span>
<span><span class="co">#&gt;   arguments:</span></span>
<span><span class="co">#&gt;     - path: a character vector of full path names; the default corresponds to the working directory, 'getwd()'. (...) [Type: string]</span></span>
<span><span class="co">#&gt;     - pattern: an optional regular expression.  Only file names which match the regular expression will be returned. [Type: unknown]</span></span>
<span><span class="co">#&gt;     - all.files: a logical value.  If 'FALSE', only the names of visible files are returned (following Unix-style visibility, that is files whose name does not start with a dot).  If 'TRUE', all file names will be returned. [Type: logical]</span></span>
<span><span class="co">#&gt;     - full.names: a logical value.  If 'TRUE', the directory path is prepended to the file names to give a relative file path.  If 'FALSE', the file names (rather than paths) are returned. [Type: logical]</span></span>
<span><span class="co">#&gt;     - recursive: logical.  Should the listing recurse into directories? [Type: logical]</span></span>
<span><span class="co">#&gt;     - ignore.case: logical.  Should pattern-matching be case-insensitive? [Type: logical]</span></span>
<span><span class="co">#&gt;     - (...)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; After you call a function, wait until you receive more information.</span></span>
<span><span class="co">#&gt; Use the information to decide your next steps or provide a final response.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; To get the files in my current directory, I will call the `list.files` function with the necessary arguments.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Here's the JSON object:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "function": "list.files",</span></span>
<span><span class="co">#&gt;   "arguments": {</span></span>
<span><span class="co">#&gt;     "path": "."</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; This will list the files in the current working directory. The argument `path` is set to `"."`, which corresponds to the current working directory, and I'm not providing any pattern or other arguments as they are optional.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Let's see what happens next...</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; function called: list.files</span></span>
<span><span class="co">#&gt; arguments used: path = .</span></span>
<span><span class="co">#&gt; result: getting_started.Rmd, getting_started.Rmd.orig, man, precompile vignettes.R, sentiment_analysis.Rmd, sentiment_analysis.Rmd.orig</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; It looks like I have a mix of files in my current directory.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The `list.files` function returned a list of file names, which are:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * `getting_started.Rmd`</span></span>
<span><span class="co">#&gt; * `getting_started.Rmd.orig`</span></span>
<span><span class="co">#&gt; * `man`</span></span>
<span><span class="co">#&gt; * `precompile vignettes.R`</span></span>
<span><span class="co">#&gt; * `sentiment_analysis.Rmd`</span></span>
<span><span class="co">#&gt; * `sentiment_analysis.Rmd.orig`</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; These files seem to be related to some kind of project or documentation.</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="code-generation-and-evaluation">Code generation and evaluation<a class="anchor" aria-label="anchor" href="#code-generation-and-evaluation"></a>
</h4>
<p><code><a href="../reference/answer_using_r.html">answer_using_r()</a></code> provides a more advanced prompt wrap,
which has various options to enable LLM code generation. R code can be
extracted, parsed for validity, and optionally be evaluated in a
dedicated R session (using the ‘callr’ package). The prompt wrap can
also be set to ‘tool mode’ (with <code>output_as_tool = TRUE</code>),
where the output of R code is returned to the LLM, so that it can be
used to formulate a final answer.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># From prompt to ggplot</span></span>
<span><span class="va">plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span></span>
<span>  <span class="st">"Create a scatter plot of miles per gallon (mpg) versus"</span>,</span>
<span>  <span class="st">" horsepower (hp) for the cars in the mtcars dataset."</span>,</span>
<span>  <span class="st">" Use different colors to represent the number of cylinders (cyl)."</span>,</span>
<span>  <span class="st">" Make the plot nice and readable,"</span>,</span>
<span>  <span class="st">" but also be creative, a little crazy, and have humour!"</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/answer_using_r.html">answer_using_r</a></span><span class="op">(</span></span>
<span>    pkgs_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ggplot2"</span><span class="op">)</span>,</span>
<span>    evaluate_code <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    return_mode <span class="op">=</span> <span class="st">"object"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span>
<span><span class="va">plot</span></span></code></pre></div>
<div class="float">
<img src="../reference/figures/answer_using_r1-1.png" alt="plot"><div class="figcaption">plot</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="creating-custom-prompt-wraps">Creating custom prompt wraps<a class="anchor" aria-label="anchor" href="#creating-custom-prompt-wraps"></a>
</h3>
<p>See <code><a href="../reference/prompt_wrap.html">prompt_wrap()</a></code> and the ‘<a href="creating_prompt_wraps.html">Creating prompt wraps</a>’ vignette
for information on how to create your own prompt wraps.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
