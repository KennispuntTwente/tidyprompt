<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Getting started • tidyprompt</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Getting started">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="../articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjarkvandemerwe/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Getting started</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tjarkvandemerwe/tidyprompt/blob/main/vignettes/getting_started.Rmd" class="external-link"><code>vignettes/getting_started.Rmd</code></a></small>
      <div class="d-none name"><code>getting_started.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tjarkvandemerwe/tidyprompt" class="external-link">tidyprompt</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="setup-an-llm-provider">Setup an LLM provider<a class="anchor" aria-label="anchor" href="#setup-an-llm-provider"></a>
</h3>
<p>‘tidyprompt’ can be used with any LLM provider capable of completing
a chat.</p>
<p>At the moment, ‘tidyprompt’ includes pre-built functions to connect
with various LLM providers, such as Ollama, OpenAI, OpenRouter, Mistral,
Groq, XAI (Grok), and Google Gemini.</p>
<p>With <code><a href="../reference/llm_provider.html">llm_provider()</a></code>, you can easily write a hook for any
other LLM provider. You could make API calls using the ‘httr’ package or
use another R package that already has a hook for the LLM provider you
want to use. If your API of choice follows the structure of the OpenAI
API, you can call <code><a href="../reference/llm_provider_openai.html">llm_provider_openai()</a></code> and change the
relevant parameters (like the URL and the API key).</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ollama running on local PC</span></span>
<span><span class="va">ollama</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"llama3.1:8b"</span><span class="op">)</span>,</span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># OpenAI API</span></span>
<span><span class="va">openai</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Various providers via OpenRouter (e.g., Anthropic)</span></span>
<span><span class="va">openrouter</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_provider_openrouter.html">llm_provider_openrouter</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"anthropic/claude-3.5-sonnet"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ... functions also included for Mistral, Groq, XAI (Grok), and Google Gemini</span></span>
<span></span>
<span><span class="co"># ... or easily create your own hook for any other LLM provider;</span></span>
<span><span class="co">#   see ?llm_provider for more information; also take a look at the source code of</span></span>
<span><span class="co">#   llm_provider_ollama() and llm_provider_openai(). For APIs that follow the structure</span></span>
<span><span class="co">#   of the OpenAI API for chat completion, you can use llm_provider_openai() and change</span></span>
<span><span class="co">#   the relevant parameters (like the url and the API key).</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="basic-prompting">Basic prompting<a class="anchor" aria-label="anchor" href="#basic-prompting"></a>
</h3>
<p>A simple string serves as the base for a prompt.</p>
<p>By adding prompt wraps, you can influence various aspects of how the
LLM handles the prompt, while verifying that the output is structured
and valid (including retries with feedback to the LLM if it is not).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi there!</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; It's nice to meet you. Is there something I can help you with, or would you like to chat?</span></span>
<span><span class="co">#&gt; [1] "It's nice to meet you. Is there something I can help you with, or would you like to chat?"</span></span></code></pre></div>
<p><code><a href="../reference/add_text.html">add_text()</a></code> is a simple example of a prompt wrap. It
simply adds some text at the end of the base prompt.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is a large language model? Explain in 10 words."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi there!</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; What is a large language model? Explain in 10 words.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Sophisticated computer program that processes and generates human-like written language.</span></span>
<span><span class="co">#&gt; [1] "Sophisticated computer program that processes and generates human-like written language."</span></span></code></pre></div>
<p>You can also construct the final prompt text, without sending it to
an LLM provider.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is a large language model? Explain in 10 words."</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;tidyprompt&gt;</span></span>
<span><span class="co">#&gt; The base prompt is modified by a prompt wrap, resulting in:</span></span>
<span><span class="co">#&gt; &gt; Hi there!</span></span>
<span><span class="co">#&gt; &gt; </span></span>
<span><span class="co">#&gt; &gt; What is a large language model? Explain in 10 words. </span></span>
<span><span class="co">#&gt; Use '&lt;tidyprompt&gt;$prompt_wraps' to show the prompt wraps.</span></span>
<span><span class="co">#&gt; Use '&lt;tidyprompt&gt;$base_prompt' to show the base prompt text.</span></span>
<span><span class="co">#&gt; Use '&lt;tidyprompt&gt; |&gt; construct_prompt_text()' to get the full prompt text.</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="retrieving-output-in-a-specific-format">Retrieving output in a specific format<a class="anchor" aria-label="anchor" href="#retrieving-output-in-a-specific-format"></a>
</h3>
<p>Using prompt wraps, you can force the LLM to return the output in a
specific format. You can also extract the output to turn it from a
character into another data type.</p>
<p>For instance, <code><a href="../reference/answer_as_integer.html">answer_as_integer()</a></code> adds a prompt wrap
which forces the LLM to reply with an integer.</p>
<p>To achieve this, the prompt wrap will add some text to the base
prompt, asking the LLM to reply with an integer. However, the prompt
wrap does more: it also will attempt to extract and validate the integer
from the LLM’s response. If extraction or validation fails, feedback is
sent back to the LLM, after which the LLM can retry answering the
prompt. Because the extraction function turns the original character
response into a numeric value, the final output from
<code><a href="../reference/send_prompt.html">send_prompt()</a></code> will also be a numeric type.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 4</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
<p>Below is an example of a prompt which will initially fail, but will
succeed after <code><a href="../reference/llm_feedback.html">llm_feedback()</a></code> and a retry.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"Please write out your reply in words, use no numbers."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span>add_instruction_to_prompt <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Please write out your reply in words, use no numbers.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Two plus two equals four.</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 4</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="adding-a-reasoning-mode-to-the-llm">Adding a reasoning mode to the LLM<a class="anchor" aria-label="anchor" href="#adding-a-reasoning-mode-to-the-llm"></a>
</h3>
<p>Prompt wraps may also be used to add a reasoning mode to the LLM. It
is hypothesized that this could improve the LLM’s performance on more
complex tasks.</p>
<p>For instance, <code><a href="../reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought()</a></code> will add
chain of thought reasoning mode to the prompt evaluation by the LLM. The
function wraps the base prompt text within a request for the LLM to
reason step by step, asking it to provide the final answer within
‘FINISH[<final answer here>]’. An extraction function then ensures only
the final answer is returned.</final></p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You are given a user's prompt.</span></span>
<span><span class="co">#&gt; To answer the user's prompt, you need to think step by step to arrive at a final answer.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ----- START OF USER'S PROMPT -----</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; ----- END OF USER'S PROMPT -----</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; What are the steps you would take to answer the user's prompt?</span></span>
<span><span class="co">#&gt; Describe your thought process in the following format:</span></span>
<span><span class="co">#&gt;   &gt;&gt; step 1: &lt;step 1 description&gt;</span></span>
<span><span class="co">#&gt;   &gt;&gt; step 2: &lt;step 2 description&gt;</span></span>
<span><span class="co">#&gt;   (etc.)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; When you are done, you must type:</span></span>
<span><span class="co">#&gt;   FINISH[&lt;put here your final answer to the user's prompt&gt;]</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Make sure your final answer follows the logical conclusion of your thought process.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; &gt;&gt; step 1: Identify that the problem is a basic arithmetic operation involving addition.</span></span>
<span><span class="co">#&gt; &gt;&gt; step 2: Recall the specific numbers involved in the operation, which are 2 and 2.</span></span>
<span><span class="co">#&gt; &gt;&gt; step 3: Perform the operation by adding the two numbers together to get their sum.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; FINISH[4]</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="giving-tools-to-the-llm-autonomous-function-calling">Giving tools to the LLM (autonomous function-calling)<a class="anchor" aria-label="anchor" href="#giving-tools-to-the-llm-autonomous-function-calling"></a>
</h3>
<p>With <code><a href="../reference/add_tools.html">add_tools()</a></code>, you can define your own R functions
and give the LLM the ability to call them and process their output. This
enables the LLM to autonomously retrieve additional information or take
other actions.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="co"># Define a function that returns fake data about the temperature in a location</span></span>
<span>  <span class="va">temperature_in_location</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span></span>
<span>    <span class="va">location</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Amsterdam"</span>, <span class="st">"Utrecht"</span>, <span class="st">"Enschede"</span><span class="op">)</span>,</span>
<span>    <span class="va">unit</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Celcius"</span>, <span class="st">"Fahrenheit"</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">location</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/match.arg.html" class="external-link">match.arg</a></span><span class="op">(</span><span class="va">location</span><span class="op">)</span></span>
<span>    <span class="va">unit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/match.arg.html" class="external-link">match.arg</a></span><span class="op">(</span><span class="va">unit</span><span class="op">)</span></span>
<span></span>
<span>    <span class="va">temperature_celcius</span> <span class="op">&lt;-</span> <span class="kw"><a href="https://rdrr.io/r/base/switch.html" class="external-link">switch</a></span><span class="op">(</span></span>
<span>      <span class="va">location</span>,</span>
<span>      <span class="st">"Amsterdam"</span> <span class="op">=</span> <span class="fl">32.55</span>,</span>
<span>      <span class="st">"Utrecht"</span> <span class="op">=</span> <span class="fl">19.8</span>,</span>
<span>      <span class="st">"Enschede"</span> <span class="op">=</span> <span class="fl">22.7</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">unit</span> <span class="op">==</span> <span class="st">"Celcius"</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">temperature_celcius</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">temperature_celcius</span> <span class="op">*</span> <span class="fl">9</span><span class="op">/</span><span class="fl">5</span> <span class="op">+</span> <span class="fl">32</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># Add documentation to the function for the LLM</span></span>
<span>  <span class="va">temperature_in_location</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/add_tools_add_documentation.html">add_tools_add_documentation</a></span><span class="op">(</span></span>
<span>    <span class="va">temperature_in_location</span>,</span>
<span>    description <span class="op">=</span> <span class="st">"Get the temperature in a location"</span>,</span>
<span>    arguments <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      location <span class="op">=</span> <span class="st">"Location, must be one of: 'Amsterdam', 'Utrecht', 'Enschede'"</span>,</span>
<span>      unit <span class="op">=</span> <span class="st">"Unit, must be one of: 'Celcius', 'Fahrenheit'"</span></span>
<span>    <span class="op">)</span>,</span>
<span>    return_value <span class="op">=</span> <span class="st">"The temperature in the specified location and unit"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Ask the LLM a question which can be answered with the function</span></span>
<span>  <span class="st">"Hi, what is the weather temperature in Enschede?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"I want to know the Celcius degrees."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_tools.html">add_tools</a></span><span class="op">(</span><span class="va">temperature_in_location</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi, what is the weather temperature in Enschede?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; I want to know the Celcius degrees.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; If you need more information, you can call functions to help you.</span></span>
<span><span class="co">#&gt; To call a function, output a JSON object with the following format:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "function": "&lt;function name&gt;",</span></span>
<span><span class="co">#&gt;   "arguments": {</span></span>
<span><span class="co">#&gt;     "&lt;argument_name&gt;": &lt;argument_value&gt;,</span></span>
<span><span class="co">#&gt;     ...</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Note: you cannot call other functions within arguments.)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The following functions are available:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   function name: temperature_in_location</span></span>
<span><span class="co">#&gt;   description: Get the temperature in a location</span></span>
<span><span class="co">#&gt;   arguments:</span></span>
<span><span class="co">#&gt;     - location: Location, must be one of: 'Amsterdam', 'Utrecht', 'Enschede'</span></span>
<span><span class="co">#&gt;     - unit: Unit, must be one of: 'Celcius', 'Fahrenheit'</span></span>
<span><span class="co">#&gt;   return value: The temperature in the specified location and unit</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; After you call a function, wait until you receive more information.</span></span>
<span><span class="co">#&gt; Use the information to decide your next steps or provide a final response.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; To get the weather temperature in Enschede, I will call the `temperature_in_location` function with the required arguments.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Here's my first action:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "function": "temperature_in_location",</span></span>
<span><span class="co">#&gt;   "arguments": {</span></span>
<span><span class="co">#&gt;     "location": "Enschede",</span></span>
<span><span class="co">#&gt;     "unit": "Celcius"</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Please wait for more information...</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; function called: temperature_in_location</span></span>
<span><span class="co">#&gt; arguments used: location = Enschede, unit = Celcius</span></span>
<span><span class="co">#&gt; result: 22.7</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; The current temperature in Enschede is 22.7°C.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; So, my response to your original question is:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 22.7</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 22</span></span>
<span><span class="co">#&gt; [1] 22</span></span></code></pre></div>
<p><code><a href="../reference/add_tools.html">add_tools()</a></code> can also be used to give the LLM access to
pre-existing functions from packages or base R. The documentation will
then be extracted from the available help file.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What are the files in my current directory?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/add_tools.html">add_tools</a></span><span class="op">(</span><span class="va">list.files</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What are the files in my current directory?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; If you need more information, you can call functions to help you.</span></span>
<span><span class="co">#&gt; To call a function, output a JSON object with the following format:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "function": "&lt;function name&gt;",</span></span>
<span><span class="co">#&gt;   "arguments": {</span></span>
<span><span class="co">#&gt;     "&lt;argument_name&gt;": &lt;argument_value&gt;,</span></span>
<span><span class="co">#&gt;     ...</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Note: you cannot call other functions within arguments.)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The following functions are available:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   function name: list.files</span></span>
<span><span class="co">#&gt;   description: List the Files in a Directory/Folder: These functions produce a character vector of the names of files</span></span>
<span><span class="co">#&gt; or directories in the named directory.</span></span>
<span><span class="co">#&gt;   arguments:</span></span>
<span><span class="co">#&gt;     - path: a character vector of full path names; the default corresponds to the working directory, 'getwd()'.  Tilde expansion (see 'path.expand') is performed.  Missing values will be ignored.  Elements with a marked encoding will be converted to the native encoding (and if that fails, considered non-existent).</span></span>
<span><span class="co">#&gt;     - pattern: an optional regular expression.  Only file names which match the regular expression will be returned.</span></span>
<span><span class="co">#&gt;     - all.files: a logical value.  If 'FALSE', only the names of visible files are returned (following Unix-style visibility, that is files whose name does not start with a dot).  If 'TRUE', all file names will be returned.</span></span>
<span><span class="co">#&gt;     - full.names: a logical value.  If 'TRUE', the directory path is prepended to the file names to give a relative file path.  If 'FALSE', the file names (rather than paths) are returned.</span></span>
<span><span class="co">#&gt;     - recursive: logical.  Should the listing recurse into directories?</span></span>
<span><span class="co">#&gt;     - ignore.case: logical.  Should pattern-matching be case-insensitive?</span></span>
<span><span class="co">#&gt;     - include.dirs: logical.  Should subdirectory names be included in recursive listings?  (They always are in non-recursive ones).</span></span>
<span><span class="co">#&gt;     - no..: logical.  Should both '"."' and '".."' be excluded also from non-recursive listings?</span></span>
<span><span class="co">#&gt;   return value: A character vector containing the names of the files in the</span></span>
<span><span class="co">#&gt; specified directories (empty if there were no files).  If a path</span></span>
<span><span class="co">#&gt; does not exist or is not a directory or is unreadable it is</span></span>
<span><span class="co">#&gt; skipped.</span></span>
<span><span class="co">#&gt; The files are sorted in alphabetical order, on the full path if</span></span>
<span><span class="co">#&gt; 'full.names = TRUE'.</span></span>
<span><span class="co">#&gt; 'list.dirs' implicitly has 'all.files = TRUE', and if 'recursive =</span></span>
<span><span class="co">#&gt; TRUE', the answer includes 'path' itself (provided it is a</span></span>
<span><span class="co">#&gt; readable directory).</span></span>
<span><span class="co">#&gt; 'dir' is an alias for 'list.files'.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; After you call a function, wait until you receive more information.</span></span>
<span><span class="co">#&gt; Use the information to decide your next steps or provide a final response.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; I can call the "list.files" function with the necessary arguments.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Here's my JSON object calling the function:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "function": "list.files",</span></span>
<span><span class="co">#&gt;   "arguments": {</span></span>
<span><span class="co">#&gt;     "path": "."</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Please wait for the output to determine my next steps.</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; function called: list.files</span></span>
<span><span class="co">#&gt; arguments used: path = .</span></span>
<span><span class="co">#&gt; result: getting_started.Rmd, getting_started.Rmd.orig, precompile vignettes.R</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; The function has returned a result.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; It seems that I have a single file in my current directory (which is represented by the '.' argument), which are:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 1. `getting_started.Rmd`</span></span>
<span><span class="co">#&gt; 2. `getting_started.Rmd.orig`</span></span>
<span><span class="co">#&gt; 3. `precompile vignettes.R`</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Since there are multiple files, and they do not match any specific pattern or condition mentioned in the problem statement, I will conclude that this is indeed the list of files in my current directory.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Therefore, my final response is:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The files in your current directory are: `getting_started.Rmd`, `getting_started.Rmd.orig`, `precompile vignettes.R`.</span></span>
<span><span class="co">#&gt; [1] "The function has returned a result.\n\nIt seems that I have a single file in my current directory (which is represented by the '.' argument), which are:\n\n1. `getting_started.Rmd`\n2. `getting_started.Rmd.orig`\n3. `precompile vignettes.R`\n\nSince there are multiple files, and they do not match any specific pattern or condition mentioned in the problem statement, I will conclude that this is indeed the list of files in my current directory.\n\nTherefore, my final response is:\n\nThe files in your current directory are: `getting_started.Rmd`, `getting_started.Rmd.orig`, `precompile vignettes.R`."</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="code-generation-and-evaluation">Code generation and evaluation<a class="anchor" aria-label="anchor" href="#code-generation-and-evaluation"></a>
</h3>
<p><code><a href="../reference/answer_as_code.html">answer_as_code()</a></code> provides a more advanced prompt wrap,
which has various options to enable LLM code generation. R code can be
extracted, parsed for validity, and optionally be evaluated in a
dedicated R session (using the ‘callr’ package). The prompt wrap can
also be set to ‘tool mode’ (with <code>output_as_tool = TRUE</code>),
where the output of R code is returned to the LLM, so that it can be
used to formulate a final answer.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># From prompt to ggplot</span></span>
<span><span class="va">plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span></span>
<span>  <span class="st">"Create a scatter plot of miles per gallon (mpg) versus"</span>,</span>
<span>  <span class="st">" horsepower (hp) for the cars in the mtcars dataset."</span>,</span>
<span>  <span class="st">" Use different colors to represent the number of cylinders (cyl)."</span>,</span>
<span>  <span class="st">" Make the plot nice and readable,"</span>,</span>
<span>  <span class="st">" but also be creative, a little crazy, and have humour!"</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/answer_as_code.html">answer_as_code</a></span><span class="op">(</span></span>
<span>    pkgs_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ggplot2"</span><span class="op">)</span>,</span>
<span>    evaluate_code <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    return_mode <span class="op">=</span> <span class="st">"object"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span>
<span><span class="va">plot</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="creating-your-own-prompt-wraps">Creating your own prompt wraps<a class="anchor" aria-label="anchor" href="#creating-your-own-prompt-wraps"></a>
</h3>
<p>Using <code><a href="../reference/prompt_wrap.html">prompt_wrap()</a></code>, you can create your own prompt
wraps. An input for <code><a href="../reference/prompt_wrap.html">prompt_wrap()</a></code> wrap may be string or a
tidyprompt object. If you pass a string, it will be automatically turned
into a tidyprompt object.</p>
<p>Under the hood, a tidyprompt object is just a list with a base prompt
(a string) and a series of prompt wraps. <code><a href="../reference/prompt_wrap.html">prompt_wrap()</a></code> adds
a new prompt wrap to the list of prompt wraps. Each prompt wrap is a
list with a modification function, an extraction function, and/or a
validation function (at least one of these functions must be present).
The modification function alters the prompt text, the extraction
function applies a transformation to the LLM’s response, and the
validation function checks if the (transformed) LLM’s response is
valid.</p>
<p>Both extraction and validation functions can return feedback to the
LLM, using <code><a href="../reference/llm_feedback.html">llm_feedback()</a></code>. When an extraction or validation
function returns this, a message is sent back to the LLM, and the LLM
can retry answering the prompt according to the feedback. Feedback
messages may be a reiteration of instruction or a specific error message
which occured during extraction or validation. When all extractions and
validations have been applied without resulting in feedback, the LLM’s
response (after transformations by the extraction functions) will be
returned. (<code><a href="../reference/send_prompt.html">send_prompt()</a></code> is responsible for executing this
process.)</p>
<p>Below is a simple example of a prompt wrap, which just adds some text
to the base prompt:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span></span>
<span>    modify_fn <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">base_prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">base_prompt</span>, <span class="st">"How are you?"</span>, sep <span class="op">=</span> <span class="st">"\n\n"</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Shorter notation of the above would be:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">x</span>, <span class="st">"How are you?"</span>, sep <span class="op">=</span> <span class="st">"\n\n"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Often times, it may be preferred to make a function which takes a
prompt and returns a wrapped prompt:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_prompt_wrap</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">modify_fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">base_prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">base_prompt</span>, <span class="st">"How are you?"</span>, sep <span class="op">=</span> <span class="st">"\n\n"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="fu"><a href="../reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span><span class="va">prompt</span>, <span class="va">modify_fn</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">my_prompt_wrap</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Take look at the source code of <code><a href="../reference/answer_as_boolean.html">answer_as_boolean()</a></code>,
which also uses extraction:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">answer_as_boolean</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span></span>
<span>    <span class="va">prompt</span>,</span>
<span>    <span class="va">true_definition</span> <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>    <span class="va">false_definition</span> <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>    <span class="va">add_instruction_to_prompt</span> <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">instruction</span> <span class="op">&lt;-</span> <span class="st">"You must answer with only TRUE or FALSE (use no other characters)."</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">true_definition</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">instruction</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">instruction</span>, <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"TRUE means: {true_definition}."</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">false_definition</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">instruction</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">instruction</span>, <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"FALSE means: {false_definition}."</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">modify_fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">original_prompt_text</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="va">add_instruction_to_prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">original_prompt_text</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"{original_prompt_text}\n\n{instruction}"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">extraction_fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">normalized</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/trimws.html" class="external-link">trimws</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">normalized</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"true"</span>, <span class="st">"false"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/logical.html" class="external-link">as.logical</a></span><span class="op">(</span><span class="va">normalized</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="../reference/llm_feedback.html">llm_feedback</a></span><span class="op">(</span><span class="va">instruction</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="fu"><a href="../reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span><span class="va">prompt</span>, <span class="va">modify_fn</span>, <span class="va">extraction_fn</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Take a look at the source code of, for instance,
<code><a href="../reference/answer_as_integer.html">answer_as_integer()</a></code>,
<code><a href="../reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought()</a></code>, and <code><a href="../reference/add_tools.html">add_tools()</a></code>
for more advanced examples of prompt wraps.</p>
<div class="section level4">
<h4 id="breaking-out-of-the-evaluation-loop">Breaking out of the evaluation loop<a class="anchor" aria-label="anchor" href="#breaking-out-of-the-evaluation-loop"></a>
</h4>
<p>In some cases, you may want to exit the extraction or validation
process early. For instance, your LLM may indicate that it is unable to
answer the prompt. In such cases, you can have your extraction or
validation function return <code><a href="../reference/llm_break.html">llm_break()</a></code>. This will cause the
evaluation loop to break, forwarding to the return statement of
<code><a href="../reference/send_prompt.html">send_prompt()</a></code>. See <code><a href="../reference/quit_if.html">quit_if()</a></code> for an example of
this.</p>
</div>
<div class="section level4">
<h4 id="extraction-versus-validation-functions">Extraction versus validation functions<a class="anchor" aria-label="anchor" href="#extraction-versus-validation-functions"></a>
</h4>
<p>Both extraction and validation functions can return
<code><a href="../reference/llm_break.html">llm_break()</a></code> or <code><a href="../reference/llm_feedback.html">llm_feedback()</a></code>. The difference
between extraction and validation functions is only that an extraction
may transform the LLM response and pass it on to the next extraction
and/or validation functions, while a validation function only checks if
the LLM response passes a logical test (without altering the response).
Thus, if you wish, you can perform validations in an extraction
function.</p>
</div>
<div class="section level4">
<h4 id="prompt-wrap-types-and-order-of-application">Prompt wrap types and order of application<a class="anchor" aria-label="anchor" href="#prompt-wrap-types-and-order-of-application"></a>
</h4>
<p>When constructing the prompt text and when evaluating a prompt,
prompt wraps are applied prompt wrap after prompt wrap (e.g., first the
extraction and validation functions of one wrap, then of the other).</p>
<p>The order in which prompt wraps are applied is important. Currently,
four types of prompt wraps are distinguished: ‘unspecified’, ‘break’,
‘mode’, and ‘tool’.</p>
<p>When constructing the prompt text, prompt wraps are applied in the
order of these types. Prompt wraps will be automatically reordered if
necesarry (keeping intact the order of prompt wraps of the same
type).</p>
<p>When evaluating the prompt, prompt wraps are applied in the reverse
order of types (i.e., first ‘tool’, then ‘mode’, then ‘break’, and
finally ‘unspecified’). This is because ‘tool’ prompt wraps may return a
value to be used in the final answer, ‘mode’ prompt wraps alter how a
LLM forms a final answer, ‘break’ prompt wraps quit evaluation early
based on a specific final answer, and ‘unspecified’ prompt wraps are the
most general type of prompt wraps which force a final answer to be in a
specific format.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
