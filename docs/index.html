<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Prompt and empower your LLM, the tidy way • tidyprompt</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Prompt and empower your LLM, the tidy way">
<meta name="description" content="The tidyprompt package allows users to prompt and empower their large language models (LLMs) in a tidy way. It provides a framework to construct LLM prompts using tidyverse-inspired piping syntax, with a library of pre-built prompt wrappers and the option to build custom ones. Additionally, it supports structured LLM output extraction and validation, with automatic feedback and retries if necessary. Moreover, it enables specific LLM reasoning modes, autonomous R function calling for LLMs, and compatibility with any LLM provider.">
<meta property="og:description" content="The tidyprompt package allows users to prompt and empower their large language models (LLMs) in a tidy way. It provides a framework to construct LLM prompts using tidyverse-inspired piping syntax, with a library of pre-built prompt wrappers and the option to build custom ones. Additionally, it supports structured LLM output extraction and validation, with automatic feedback and retries if necessary. Moreover, it enables specific LLM reasoning modes, autonomous R function calling for LLMs, and compatibility with any LLM provider.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="articles/example_usage.html">Example usage</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjarkvandemerwe/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="tidyprompt">tidyprompt<a class="anchor" aria-label="anchor" href="#tidyprompt"></a>
</h1></div>
<!-- badges: start -->

<p>‘tidyprompt’ is an R package to prompt and empower your large language models (LLMs), the tidy way.</p>
<p>Key features of ‘tidyprompt’ are:</p>
<ul>
<li><p><strong>tidy prompting</strong>: Quickly and elegantly construct prompts for LLMs, using piping syntax (inspired by the ‘tidyverse’). Wrap a base prompt in prompt wraps to influence how the LLM handles the prompt. A library of pre-built prompt wraps is included, but you can also write your own.</p></li>
<li><p><strong>structured output</strong>: Extract structured output from the LLM’s response, and validate it. Automatic retries with feedback to the LLM, if the output is not as expected.</p></li>
<li><p><strong>reasoning modes</strong>: Make your LLM answer in a specific mode, such as chain-of-thought or ReAct (Reasoning and Acting) modes.</p></li>
<li><p><strong>function calling</strong>: Give your LLM the ability to autonomously call R functions (‘tools’). With this, the LLM can retrieve information or take other actions. ‘tidyprompt’ also supports R code generation and evaluation, allowing LLMs to run R code.</p></li>
<li><p><strong>compatible with all LLM providers</strong>: Usable with any LLM provider that supports chat completion. Use included LLM providers such as Ollama (local PC/on your own server), OpenAI, OpenRouter (offering various providers, including Anthropic), Mistral, Groq, XAI (Grok), or Google Gemini. Or easily write your own hook for any other LLM provider.</p></li>
</ul>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install the development version of tidyprompt from <a href="https://github.com/tjarkvandemerwe/tidyprompt" class="external-link">GitHub</a> with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("remotes")</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"tjarkvandemerwe/tidyprompt"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="example-usage">Example usage<a class="anchor" aria-label="anchor" href="#example-usage"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tjarkvandemerwe/tidyprompt" class="external-link">tidyprompt</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="setup-an-llm-provider">Setup an LLM provider<a class="anchor" aria-label="anchor" href="#setup-an-llm-provider"></a>
</h3>
<p>‘tidyprompt’ can be used with any LLM provider capable of completing a chat.</p>
<p>At the moment, ‘tidyprompt’ includes pre-built functions to connect with various LLM providers, such as Ollama, OpenAI, OpenRouter, Mistral, Groq, XAI (Grok), and Google Gemini.</p>
<p>With <code><a href="reference/llm_provider.html">llm_provider()</a></code>, you can easily write a hook for any other LLM provider. You could make API calls using the ‘httr’ package or use another R package that already has a hook for the LLM provider you want to use. If your API of choice follows the structure of the OpenAI API, you can call <code><a href="reference/llm_provider_openai.html">llm_provider_openai()</a></code> and change the relevant parameters (like the URL and the API key).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ollama running on local PC</span></span>
<span><span class="va">ollama</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"llama3.1:8b"</span><span class="op">)</span>,</span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># OpenAI API</span></span>
<span><span class="va">openai</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_provider_openai.html">llm_provider_openai</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Various providers via OpenRouter (e.g., Anthropic)</span></span>
<span><span class="va">openrouter</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/llm_provider_openrouter.html">llm_provider_openrouter</a></span><span class="op">(</span></span>
<span>  parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"anthropic/claude-3.5-sonnet"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ... functions also included for Mistral, Groq, XAI (Grok), and Google Gemini</span></span>
<span></span>
<span><span class="co"># ... or easily create your own hook for any other LLM provider;</span></span>
<span><span class="co">#   see ?llm_provider for more information; also take a look at the source code of</span></span>
<span><span class="co">#   llm_provider_ollama() and llm_provider_openai(). For APIs that follow the structure</span></span>
<span><span class="co">#   of the OpenAI API for chat completion, you can use llm_provider_openai() and change</span></span>
<span><span class="co">#   the relevant parameters (like the url and the API key).</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="basic-prompting">Basic prompting<a class="anchor" aria-label="anchor" href="#basic-prompting"></a>
</h3>
<p>A simple string serves as the base for a prompt.</p>
<p>By adding prompt wraps, you can influence various aspects of how the LLM handles the prompt, while verifying that the output is structured and valid (including retries with feedback to the LLM if it is not).</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi there!</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; It's nice to meet you. Is there something I can help you with or would you like to chat?</span></span>
<span><span class="co">#&gt; [1] "It's nice to meet you. Is there something I can help you with or would you like to chat?"</span></span></code></pre></div>
<p><code>add_text</code> is a simple example of a prompt wrap. It simply adds some text at the end of the base prompt.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is a large language model? Explain in 10 words."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi there!</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; What is a large language model? Explain in 10 words.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Advanced computer program that understands and generates human-like written text.</span></span>
<span><span class="co">#&gt; [1] "Advanced computer program that understands and generates human-like written text."</span></span></code></pre></div>
<p>You can also construct the final prompt text, without sending it to an LLM provider.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is a large language model? Explain in 10 words."</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;tidyprompt&gt;</span></span>
<span><span class="co">#&gt; The base prompt is modified by a prompt wrap, resulting in:</span></span>
<span><span class="co">#&gt; &gt; Hi there!</span></span>
<span><span class="co">#&gt; &gt; </span></span>
<span><span class="co">#&gt; &gt; What is a large language model? Explain in 10 words. </span></span>
<span><span class="co">#&gt; Use '&lt;tidyprompt&gt;$prompt_wraps' to show the prompt wraps.</span></span>
<span><span class="co">#&gt; Use '&lt;tidyprompt&gt;$base_prompt' to show the base prompt text.</span></span>
<span><span class="co">#&gt; Use '&lt;tidyprompt&gt; |&gt; construct_prompt_text()' to get the full prompt text.</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="retrieving-output-in-a-specific-format">Retrieving output in a specific format<a class="anchor" aria-label="anchor" href="#retrieving-output-in-a-specific-format"></a>
</h3>
<p>Using prompt wraps, you can force the LLM to return the output in a specific format. You can also extract the output to turn it from a character into another data type.</p>
<p>For instance, <code><a href="reference/answer_as_integer.html">answer_as_integer()</a></code> adds a prompt wrap which forces the LLM to reply with an integer.</p>
<p>To achieve this, the prompt wrap will add some text to the base prompt, asking the LLM to reply with an integer. However, the prompt wrap does more: it also will attempt to extract and validate the integer from the LLM’s response. If extraction or validation fails, feedback is sent back to the LLM, after which the LLM can retry answering the prompt. Because the extraction function turns the original character response into a numeric value, the final output from <code><a href="reference/send_prompt.html">send_prompt()</a></code> will also be a numeric type.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 4</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
<p>Below is an example of a prompt which will initially fail, but will succeed after <code><a href="reference/llm_feedback.html">llm_feedback()</a></code> and a retry.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"Please write out your reply in words, use no numbers."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span>add_instruction_to_prompt <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Please write out your reply in words, use no numbers.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; Two plus two equals four.</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 4</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="adding-a-reasoning-mode-to-the-llm">Adding a reasoning mode to the LLM<a class="anchor" aria-label="anchor" href="#adding-a-reasoning-mode-to-the-llm"></a>
</h3>
<p>Prompt wraps may also be used to add a reasoning mode to the LLM. It is hypothesized that this could improve the LLM’s performance on more complex tasks.</p>
<p>For instance, <code><a href="reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought()</a></code> will add chain of thought reasoning mode to the prompt evaluation by the LLM. The function wraps the base prompt text within a request for the LLM to reason step by step, asking it to provide the final answer within ‘FINISH[<final answer here>]’. An extraction function then ensures only the final answer is returned.</final></p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="st">"What is 2 + 2?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You are given a user's prompt.</span></span>
<span><span class="co">#&gt; To answer the user's prompt, you need to think step by step to arrive at a final answer.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ----- START OF USER'S PROMPT -----</span></span>
<span><span class="co">#&gt; What is 2 + 2?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; ----- END OF USER'S PROMPT -----</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; What are the steps you would take to answer the user's prompt?</span></span>
<span><span class="co">#&gt; Describe your thought process in the following format:</span></span>
<span><span class="co">#&gt;   &gt;&gt; step 1: &lt;step 1 description&gt;</span></span>
<span><span class="co">#&gt;   &gt;&gt; step 2: &lt;step 2 description&gt;</span></span>
<span><span class="co">#&gt;   (etc.)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; When you are done, you must type:</span></span>
<span><span class="co">#&gt; FINISH[&lt;put here your final answer to the user's prompt&gt;]</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Make sure your final answer follows the logical conclusion of your thought process.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; &gt;&gt; step 1: Identify the mathematical operation requested in the prompt, which is addition.</span></span>
<span><span class="co">#&gt; The prompt asks for the sum of 2 and 2.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; &gt;&gt; step 2: Recall the basic arithmetic fact that 2 + 2 equals a specific number.</span></span>
<span><span class="co">#&gt; This is a fundamental math concept that can be recalled from memory or learned through experience.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; &gt;&gt; step 3: Apply this knowledge to determine that the sum of 2 and 2 is indeed 4.</span></span>
<span><span class="co">#&gt; The numerical value of 4 is derived directly from knowing that 2 added to itself results in a total count of four items, quantities, or values.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; FINISH4</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Error, could not parse your final answer.</span></span>
<span><span class="co">#&gt; Please type: 'FINISH[&lt;put here your final answer to the original prompt&gt;]'</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; FINISH[4]</span></span>
<span><span class="co">#&gt; [1] 4</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="giving-tools-to-the-llm-autonomous-function-calling">Giving tools to the LLM (autonomous function-calling)<a class="anchor" aria-label="anchor" href="#giving-tools-to-the-llm-autonomous-function-calling"></a>
</h3>
<p>With <code><a href="reference/add_tools.html">add_tools()</a></code>, you can define your own R functions and give the LLM the ability to call them and process their output. This enables the LLM to autonomously retrieve additional information or take other actions.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="co"># Define a function that returns fake data about the temperature in a location</span></span>
<span>  <span class="va">temperature_in_location</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span></span>
<span>    <span class="va">location</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Amsterdam"</span>, <span class="st">"Utrecht"</span>, <span class="st">"Enschede"</span><span class="op">)</span>,</span>
<span>    <span class="va">unit</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Celcius"</span>, <span class="st">"Fahrenheit"</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co">#' llm_tool::name temperature_in_location</span></span>
<span>    <span class="co">#'</span></span>
<span>    <span class="co">#' llm_tool::description Get the temperature in a location</span></span>
<span>    <span class="co">#'</span></span>
<span>    <span class="co">#' llm_tool::param location Location, must be one of: "Amsterdam", "Utrecht", "Enschede"</span></span>
<span>    <span class="co">#' llm_tool::param unit Unit, must be one of: "Celcius", "Fahrenheit"</span></span>
<span>    <span class="co">#'</span></span>
<span>    <span class="co">#' llm_tool::return The temperature in the specified location and unit</span></span>
<span>    <span class="co">#'</span></span>
<span>    <span class="co">#' llm_tool::example</span></span>
<span>    <span class="co">#' temperature_in_location("Amsterdam", "Fahrenheit")</span></span>
<span></span>
<span>    <span class="co"># As shown above, one can use docstring-like text to document the function.</span></span>
<span>    <span class="co">#   This will provide the LLM information on what the function does,</span></span>
<span>    <span class="co">#   and how it should be used.</span></span>
<span></span>
<span>    <span class="va">location</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/match.arg.html" class="external-link">match.arg</a></span><span class="op">(</span><span class="va">location</span><span class="op">)</span></span>
<span>    <span class="va">unit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/match.arg.html" class="external-link">match.arg</a></span><span class="op">(</span><span class="va">unit</span><span class="op">)</span></span>
<span></span>
<span>    <span class="va">temperature_celcius</span> <span class="op">&lt;-</span> <span class="kw"><a href="https://rdrr.io/r/base/switch.html" class="external-link">switch</a></span><span class="op">(</span></span>
<span>      <span class="va">location</span>,</span>
<span>      <span class="st">"Amsterdam"</span> <span class="op">=</span> <span class="fl">32.5</span>,</span>
<span>      <span class="st">"Utrecht"</span> <span class="op">=</span> <span class="fl">19.8</span>,</span>
<span>      <span class="st">"Enschede"</span> <span class="op">=</span> <span class="fl">22.7</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">unit</span> <span class="op">==</span> <span class="st">"Celcius"</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">temperature_celcius</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">temperature_celcius</span> <span class="op">*</span> <span class="fl">9</span><span class="op">/</span><span class="fl">5</span> <span class="op">+</span> <span class="fl">32</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># Ask the LLM a question which can be answered with the function</span></span>
<span>  <span class="st">"Hi, what is the weather temperature in Enschede?"</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/add_text.html">add_text</a></span><span class="op">(</span><span class="st">"I want to know the Celcius degrees."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/add_tools.html">add_tools</a></span><span class="op">(</span><span class="va">temperature_in_location</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; Hi, what is the weather temperature in Enschede?</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; I want to know the Celcius degrees.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; If you need more information, you can call functions to help you.</span></span>
<span><span class="co">#&gt; To call a function, type:</span></span>
<span><span class="co">#&gt;   FUNCTION[&lt;function name here&gt;](&lt;argument 1&gt;, &lt;argument 2&gt;, etc...)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The following functions are available:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; function name: temperature_in_location</span></span>
<span><span class="co">#&gt; description: Get the temperature in a location</span></span>
<span><span class="co">#&gt; arguments:</span></span>
<span><span class="co">#&gt;     - location: Location, must be one of: "Amsterdam", "Utrecht", "Enschede"</span></span>
<span><span class="co">#&gt;     - unit: Unit, must be one of: "Celcius", "Fahrenheit"</span></span>
<span><span class="co">#&gt; return value: The temperature in the specified location and unit</span></span>
<span><span class="co">#&gt; example usage: FUNCTION[temperature_in_location]("Amsterdam", "Fahrenheit")</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; After you call a function, wait until you receive more information.</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; I'll call the `temperature_in_location` function with the necessary arguments.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; FUNCTION[temperature_in_location]("Enschede", "Celcius")</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; function called: temperature_in_location</span></span>
<span><span class="co">#&gt; arguments used: location = Enschede, unit = Celcius</span></span>
<span><span class="co">#&gt; result: 22.7</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; The current temperature in Enschede is 22.7°C.</span></span>
<span><span class="co">#&gt; --- Sending request to LLM provider (llama3.1:8b): ---</span></span>
<span><span class="co">#&gt; You must answer with only an integer (use no other characters).</span></span>
<span><span class="co">#&gt; --- Receiving response from LLM provider: ---</span></span>
<span><span class="co">#&gt; 22</span></span>
<span><span class="co">#&gt; [1] 22</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="code-generation-and-evaluation">Code generation and evaluation<a class="anchor" aria-label="anchor" href="#code-generation-and-evaluation"></a>
</h3>
<p><code><a href="reference/answer_as_code.html">answer_as_code()</a></code> provides a more advanced prompt wrap, which has various options to enable LLM code generation. R code can be extracted, parsed for validity, and optionally be evaluated in a dedicated R session (using the ‘callr’ package). The prompt wrap can also be set to ‘tool mode’ (with <code>output_as_tool = TRUE</code>), where the output of R code is returned to the LLM, so that it can be used to formulate a final answer.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># From prompt to ggplot</span></span>
<span><span class="va">plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span></span>
<span>  <span class="st">"Create a scatter plot of miles per gallon (mpg) versus"</span>,</span>
<span>  <span class="st">" horsepower (hp) for the cars in the mtcars dataset."</span>,</span>
<span>  <span class="st">" Use different colors to represent the number of cylinders (cyl)."</span>,</span>
<span>  <span class="st">" Make the plot nice and readable,"</span>,</span>
<span>  <span class="st">" but also be creative, a little crazy, and have humour!"</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/answer_as_code.html">answer_as_code</a></span><span class="op">(</span></span>
<span>    pkgs_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ggplot2"</span><span class="op">)</span>,</span>
<span>    evaluate_code <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    return_mode <span class="op">=</span> <span class="st">"object"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span>
<span><span class="va">plot</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="creating-your-own-prompt-wraps">Creating your own prompt wraps<a class="anchor" aria-label="anchor" href="#creating-your-own-prompt-wraps"></a>
</h3>
<p>Using <code><a href="reference/prompt_wrap.html">prompt_wrap()</a></code>, you can create your own prompt wraps. An input for <code><a href="reference/prompt_wrap.html">prompt_wrap()</a></code> wrap may be string or a tidyprompt object. If you pass a string, it will be automatically turned into a tidyprompt object.</p>
<p>Under the hood, a tidyprompt object is just a list with a base prompt (a string) and a series of prompt wraps. <code><a href="reference/prompt_wrap.html">prompt_wrap()</a></code> adds a new prompt wrap to the list of prompt wraps. Each prompt wrap is a list with a modification function, an extraction function, and/or a validation function (at least one of these functions must be present). The modification function alters the prompt text, the extraction function applies a transformation to the LLM’s response, and the validation function checks if the (transformed) LLM’s response is valid.</p>
<p>Both extraction and validation functions can return feedback to the LLM, using <code><a href="reference/llm_feedback.html">llm_feedback()</a></code>. When an extraction or validation function returns this, a message is sent back to the LLM, and the LLM can retry answering the prompt according to the feedback. Feedback messages may be a reiteration of instruction or a specific error message which occured during extraction or validation. When all extractions and validations have been applied without resulting in feedback, the LLM’s response (after transformations by the extraction functions) will be returned. (<code><a href="reference/send_prompt.html">send_prompt()</a></code> is responsible for executing this process.)</p>
<p>Below is a simple example of a prompt wrap, which just adds some text to the base prompt:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span></span>
<span>    modify_fn <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">base_prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">base_prompt</span>, <span class="st">"How are you?"</span>, sep <span class="op">=</span> <span class="st">"\n\n"</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Shorter notation of the above would be:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">x</span>, <span class="st">"How are you?"</span>, sep <span class="op">=</span> <span class="st">"\n\n"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Often times, it may be preferred to make a function which takes a prompt and returns a wrapped prompt:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_prompt_wrap</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">modify_fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">base_prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">base_prompt</span>, <span class="st">"How are you?"</span>, sep <span class="op">=</span> <span class="st">"\n\n"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="fu"><a href="reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span><span class="va">prompt</span>, <span class="va">modify_fn</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="st">"Hi there!"</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">my_prompt_wrap</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Take look at the source code of <code>answer_as_boolean</code>, which also uses extraction:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">answer_as_boolean</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span></span>
<span>    <span class="va">prompt</span>,</span>
<span>    <span class="va">true_definition</span> <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>    <span class="va">false_definition</span> <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>    <span class="va">add_instruction_to_prompt</span> <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">instruction</span> <span class="op">&lt;-</span> <span class="st">"You must answer with only TRUE or FALSE (use no other characters)."</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">true_definition</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">instruction</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">instruction</span>, <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"TRUE means: {true_definition}."</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">false_definition</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">instruction</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">instruction</span>, <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"FALSE means: {false_definition}."</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Define modification/extraction/validation functions:</span></span>
<span>  <span class="va">modify_fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">original_prompt_text</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="va">add_instruction_to_prompt</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">original_prompt_text</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"{original_prompt_text}\n\n{instruction}"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">extraction_fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">normalized</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/trimws.html" class="external-link">trimws</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">normalized</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"true"</span>, <span class="st">"false"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/logical.html" class="external-link">as.logical</a></span><span class="op">(</span><span class="va">normalized</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu"><a href="reference/llm_feedback.html">llm_feedback</a></span><span class="op">(</span><span class="va">instruction</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="fu"><a href="reference/prompt_wrap.html">prompt_wrap</a></span><span class="op">(</span><span class="va">prompt</span>, <span class="va">modify_fn</span>, <span class="va">extraction_fn</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Take a look at the source code of, for instance, <code><a href="reference/answer_as_integer.html">answer_as_integer()</a></code>, <code><a href="reference/answer_by_chain_of_thought.html">answer_by_chain_of_thought()</a></code>, and <code><a href="reference/add_tools.html">add_tools()</a></code> for more advanced examples of prompt wraps.</p>
<div class="section level4">
<h4 id="breaking-out-of-the-evaluation-loop">Breaking out of the evaluation loop<a class="anchor" aria-label="anchor" href="#breaking-out-of-the-evaluation-loop"></a>
</h4>
<p>In some cases, you may want to exit the extraction or validation process early. For instance, your LLM may indicate that it is unable to answer the prompt. In such cases, you can have your extraction or validation function return <code>llm_exit()</code>. This will cause the evaluation loop to break, forwarding to the return statement of <code><a href="reference/send_prompt.html">send_prompt()</a></code>. See <code><a href="reference/quit_if.html">quit_if()</a></code> for an example of this.</p>
</div>
<div class="section level4">
<h4 id="extraction-versus-validation-functions">Extraction versus validation functions<a class="anchor" aria-label="anchor" href="#extraction-versus-validation-functions"></a>
</h4>
<p>Both extraction and validation functions can return <code>llm_exit()</code> or <code><a href="reference/llm_feedback.html">llm_feedback()</a></code>. The difference between extraction and validation functions is only that an extraction may transform the LLM response and pass it on to the next extraction and/or validation functions, while a validation function only checks if the LLM response passes a logical test (without altering the response). Thus, if you wish, you can perform validations in an extraction function.</p>
</div>
<div class="section level4">
<h4 id="order-in-which-prompt-wraps-are-applied">Order in which prompt wraps are applied<a class="anchor" aria-label="anchor" href="#order-in-which-prompt-wraps-are-applied"></a>
</h4>
<p>When constructing the prompt text and when evaluating a prompt, prompt wraps are applied prompt wrap after prompt wrap (e.g., first the extraction and validation functions of one wrap, then of the other).</p>
<p>The order in which prompt wraps are applied is important. Currently, four types of prompt wraps are distinguished: ‘unspecified’, ‘break’, ‘mode’, and ‘tool’.</p>
<p>When constructing the prompt text, prompt wraps are applied in the order of these types. Prompt wraps will be automatically reordered if necesarry (keeping intact the order of prompt wraps of the same type).</p>
<p>When evaluating the prompt, prompt wraps are applied in the reverse order of types (i.e., first ‘tool’, then ‘mode’, then ‘break’, and finally ‘unspecified’). This is because ‘tool’ prompt wraps may return a value to be used in the final answer, ‘mode’ prompt wraps alter how a LLM forms a final answer, ‘break’ prompt wraps quit evaluation early based on a specific final answer, and ‘unspecified’ prompt wraps are the most general type of prompt wraps which force a final answer to be in a specific format.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="more-information-and-contributing">More information and contributing<a class="anchor" aria-label="anchor" href="#more-information-and-contributing"></a>
</h2>
<p>‘tidyprompt’ is under active development by Luka Koning (<a href="mailto:l.koning@kennispunttwente.nl" class="email">l.koning@kennispunttwente.nl</a>) and Tjark van de Merwe (<a href="mailto:t.vandemerwe@kennispunttwente.nl" class="email">t.vandemerwe@kennispunttwente.nl</a>). Note that in this stage, the package is not yet fully stable and its architecture is subject to change.</p>
<p>If you encounter issues, please open an issue in the GitHub repository. You are welcome to contribute to the package by opening a pull request. If you have any questions or suggestions, you can also reach us via e-mail.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/tjarkvandemerwe/tidyprompt/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/tjarkvandemerwe/tidyprompt/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3) | file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing tidyprompt</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Luka Koning <br><small class="roles"> Author, maintainer, copyright holder </small>  </li>
<li>Tjark Van de Merwe <br><small class="roles"> Author, copyright holder </small>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/tjarkvandemerwe/tidyprompt/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/tjarkvandemerwe/tidyprompt/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
