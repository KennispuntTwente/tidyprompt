% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/send_prompt.R
\name{send_prompt}
\alias{send_prompt}
\title{Send a prompt to a LLM provider}
\usage{
send_prompt(
  prompt,
  llm_provider = create_ollama_llm_provider(),
  max_interactions = 10,
  clean_chat_history = TRUE,
  verbose = getOption("tidyprompt.verbose", TRUE),
  stream = TRUE,
  return_mode = c("only_response", "full")
)
}
\arguments{
\item{prompt}{A prompt object or a single string}

\item{llm_provider}{'llm_provider' object (default is 'ollama')}

\item{max_interactions}{Maximum number of interactions before stopping}

\item{clean_chat_history}{If the chat history should be cleaned after each
interaction. Default is TRUE. Cleaning the chat history means that only the
first and last message from the user, the last message from the assistant,
and all messages from the system are used when requesting a new answer from
the LLM; keeping the context window clean may increase the LLM's performance.}

\item{verbose}{If the interaction with the LLM provider should be printed
to the console. Default is TRUE.}

\item{stream}{If the interaction with the LLM provider should be streamed.
Default is TRUE. This setting only be used if the LLM provider already has a
'stream' parameter (which indicates there is support for streaming).}

\item{return_mode}{One of 'full' or 'only_response'. If 'only_response',
the function will only return the response (or NULL if unsuccessful).
If 'full', the function will return a list with the following elements:
'success', 'response' (if successful), 'failed_response' (if unsuccessful),
'chat_history', 'start_time', 'end_time', and 'duration_seconds'.}
}
\value{
...
}
\description{
Send a prompt to a LLM provider
}
