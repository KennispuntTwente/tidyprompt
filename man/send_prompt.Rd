% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/send_prompt.R
\name{send_prompt}
\alias{send_prompt}
\title{Send a prompt or \code{\link[=tidyprompt]{tidyprompt()}} to a LLM provider}
\usage{
send_prompt(
  prompt,
  llm_provider = llm_provider_ollama(),
  max_interactions = 10,
  clean_chat_history = TRUE,
  verbose = getOption("tidyprompt.verbose", TRUE),
  stream = getOption("tidyprompt.stream", TRUE),
  return_mode = c("only_response", "full")
)
}
\arguments{
\item{prompt}{A string or a \code{\link[=tidyprompt]{tidyprompt()}} object}

\item{llm_provider}{\code{\link[=llm_provider]{llm_provider()}} object (default is \code{\link[=llm_provider_ollama]{llm_provider_ollama()}})}

\item{max_interactions}{Maximum number of interactions allowed with the
LLM provider. Default is 10. If the maximum number of interactions is reached
without a successful response, 'NULL' is returned as the response (see return
value)}

\item{clean_chat_history}{If the chat history should be cleaned after each
interaction. Cleaning the chat history means that only the
first and last message from the user, the last message from the assistant,
and all messages from the system are used when requesting a new answer from
the LLM; keeping the context window clean may increase the LLM's performance}

\item{verbose}{If the interaction with the LLM provider should be printed
to the console}

\item{stream}{If the interaction with the LLM provider should be streamed.
This setting only be used if the LLM provider already has a
'stream' parameter (which indicates there is support for streaming)}

\item{return_mode}{One of 'full' or 'only_response'. See return value}
}
\value{
If return mode 'only_response',the function will only return the LLM response
after extraction and validation functions have been applied (NULL is returned
when unsucessful after the maximum number of interactions). If return mode 'full',
the function, the function will return a list with the following elements:
'success' (logical indicating if all extractions and validations were successful
within the maximum number of interactions), 'response' (the LLM response
after extraction and validation functions have been applied; NULL if
unsuccesful); 'failed_response' (if unsuccessful, the LLM response
after the maximum number of interactions; NULL if successful), 'chat_history'
(a dataframe with the full chat history which led to the final response),
'chat_history_clean' (a dataframe with the cleaned chat history which led to
the final response; here, only the first and last message from the user, the
last message from the assistant, and all messages from the system are kept),
'start_time' (the time when the function was called), 'end_time' (the time
when the function ended), 'duration_seconds' (the duration of the function in
seconds), and 'http_list' (a list with all HTTP requests and full responses made
for chat completions). When using 'full' and you want to access a specific
element during (base R) piping, you can use the '\code{\link[=extract_from_return_list]{extract_from_return_list()}}'
function to assist in this
}
\description{
This function is responsible for sending strings or \code{\link[=tidyprompt]{tidyprompt()}} objects,
including their prompt wraps, to a LLM provider (see \code{\link[=llm_provider]{llm_provider()}}) for evaluation.
The function will interact with the LLM provider until a successful response
is received or the maximum number of interactions is reached. The function will
apply extraction and validation functions to the LLM response, as specified
in the prompt wraps (see \code{\link[=prompt_wrap]{prompt_wrap()}}). If the maximum number of interactions
}
\seealso{
\code{\link[=tidyprompt]{tidyprompt()}}, \code{\link[=prompt_wrap]{prompt_wrap()}}, \code{\link[=llm_provider]{llm_provider()}}, \code{\link[=llm_provider_ollama]{llm_provider_ollama()}},
\code{\link[=llm_provider_openai]{llm_provider_openai()}}, \code{\link[=llm_provider_openrouter]{llm_provider_openrouter()}}
}
\concept{prompt_evaluation}
