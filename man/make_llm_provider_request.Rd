% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_provider.R
\name{make_llm_provider_request}
\alias{make_llm_provider_request}
\title{Make a request to an LLM provider}
\usage{
make_llm_provider_request(
  url,
  headers = NULL,
  body,
  stream = NULL,
  verbose = getOption("tidyprompt.verbose", TRUE),
  api_type = c("openai", "ollama")
)
}
\arguments{
\item{url}{The URL of the LLM provider API endpoint}

\item{headers}{A named list of headers to be passed to the API (can be NULL)}

\item{body}{The body of the POST request}

\item{stream}{A logical indicating whether the API should stream responses}

\item{verbose}{A logical indicating whether the interaction with the LLM provider
should be printed to the console. Default is TRUE.}

\item{api_type}{The type of API to use. Currently, "openai" and "ollama" have been implemented
in this function. "openai" should also work with other similar APIs for chat completion}
}
\value{
A list with the role and content of the response from the LLM provider
}
\description{
Helper function to handle making requests to LLM providers, to be used
within a complete_chat() function for a LLM provider.
}
