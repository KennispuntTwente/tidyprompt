% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_provider.R
\name{make_llm_provider_request}
\alias{make_llm_provider_request}
\title{Make a request to an LLM provider}
\usage{
make_llm_provider_request(
  url,
  headers = NULL,
  body,
  stream = NULL,
  verbose = getOption("tidyprompt.verbose", TRUE),
  stream_api_type = c("openai", "ollama")
)
}
\arguments{
\item{url}{The URL of the LLM provider API endpoint}

\item{headers}{A list of headers to be passed to the API (can be NULL)}

\item{body}{The body of the POST request}

\item{stream}{A logical indicating whether the API should stream responses}

\item{verbose}{A logical indicating whether the interaction with the LLM provider
should be printed to the console. Default is TRUE.}

\item{stream_api_type}{The type of API to use; specifically required to handle streaming.
Currently, "openai" and "ollama" have been implemented. "openai" should also work
with other similar APIs for chat completion. If your API handles streaming in a
different way, you may need to implement your own version of this function
(you are then encouraged to submit a pull request to the GitHub repo of 'tidyprompt').}
}
\value{
A list with the role and content of the response from the LLM provider
}
\description{
Helper function to handle making requests to LLM providers, to be used
within a complete_chat() function for a LLM provider.
}
\examples{
# Example creation of an OpenAI provider:
llm_provider_openai <- function(
    parameters = list(
      model = "gpt-4o-mini",
      stream = getOption("tidyprompt.stream", TRUE)
    ),
    verbose = getOption("tidyprompt.verbose", TRUE),
    url = "https://api.openai.com/v1/chat/completions",
    api_key = Sys.getenv("OPENAI_API_KEY")
) {
  complete_chat <- function(chat_history) {
    headers <- c(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", self$api_key)
    )

    # Prepare the body by converting chat_history dataframe to list of lists
    body <- list(
      messages = lapply(seq_len(nrow(chat_history)), function(i) {
        list(role = chat_history$role[i], content = chat_history$content[i])
      })
    )

    # Append all other parameters to the body
    for (name in names(self$parameters))
      body[[name]] <- self$parameters[[name]]

    make_llm_provider_request(
      url = self$url,
      headers = headers,
      body = body,
      stream = self$parameters$stream,
      verbose = self$verbose,
      stream_api_type = "openai"
    )
  }

  return(llm_provider$new(
    complete_chat_function = complete_chat,
    parameters = parameters,
    verbose = verbose,
    url = url,
    api_key = api_key
  ))
}
}
\seealso{
Other llm_provider: 
\code{\link{llm_provider}},
\code{\link{llm_provider_google_gemini}()},
\code{\link{llm_provider_groq}()},
\code{\link{llm_provider_mistral}()},
\code{\link{llm_provider_ollama}()},
\code{\link{llm_provider_openai}()},
\code{\link{llm_provider_openrouter}()},
\code{\link{llm_provider_xai}()}
}
\concept{llm_provider}
