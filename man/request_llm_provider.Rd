% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/request_llm_provider.R
\name{request_llm_provider}
\alias{request_llm_provider}
\title{Make a request to an LLM provider}
\usage{
request_llm_provider(
  chat_history,
  request,
  stream = NULL,
  verbose = getOption("tidyprompt.verbose", TRUE),
  api_type = c("openai", "ollama")
)
}
\arguments{
\item{chat_history}{A data frame with 'role' and 'content' columns}

\item{request}{A 'httr2' request object with the URL, headers, and body}

\item{stream}{Logical indicating whether the API should stream responses.}

\item{verbose}{Logical indicating whether interactions should be printed to the console.}

\item{api_type}{API type, one of "openai" or "ollama".}
}
\value{
A list with the role, content, http_request, and http_response.
}
\description{
Refactored into smaller helper functions:
\itemize{
\item \code{req_llm_handle_error()}: Handle and report errors.
\item \code{req_llm_stream()}: Perform streaming requests.
\item \code{req_llm_non_stream()}: Perform non-streaming requests.
\item \code{parse_ollama_stream_chunk()}: Parse individual Ollama streaming chunks.
\item \code{parse_openai_stream_chunk()}: Parse individual OpenAI streaming chunks.
\item \code{append_or_update_tool_calls()}: Handle appending/updating tool calls for OpenAI.
}
}
