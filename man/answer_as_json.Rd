% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/answer_as_json.R
\name{answer_as_json}
\alias{answer_as_json}
\title{Make LLM answer as JSON}
\usage{
answer_as_json(
  prompt,
  type = c("text-based", "ollama", "openai"),
  schema = NULL,
  schema_strict = FALSE,
  schema_in_prompt_as = c("example", "schema")
)
}
\arguments{
\item{prompt}{A single string or a \code{\link[=tidyprompt]{tidyprompt()}} object}

\item{type}{The way in which the JSON response will be enforced.
If using "text-based" (default), the JSON response will be extracted from the LLM response
and validated within an extraction function.
If using "openai", relevant parameters will be added to the prompt through which
the API will ensure the response is a valid JSON object. When not providing
a schema, a request for a JSON object is added to the prompt as is required
by the OpenAI API.
If using "ollama", the parameter 'format' will be set to 'json' in the API request
(enforcing a JSON response). Besides that, the handling is the same as "text-based".
As recommended by Ollama, an request for a JSON object is added to the prompt.
Note that Ollama does not yet have native support for JSON schema validation, so the schema will be
validated within the extraction function by the 'jsonvalidate' package.
"text-based" is more generally applicable, while "openai" may be more efficient but
specific to the conditions of the API (note that APIs besides the OpenAI API
may follow the same structure, in which case "openai" may also be used for those APIs)}

\item{schema}{(optional) A list representing a JSON schema object that the response must match.
If provided, when using "text-based", the schema will be added to the original prompt and
the response will be validated against the schema with the 'jsonvalidate' package.
If using "openai", the schema will be added to the API request parameters and
the API will ensure the response matches the schema. See examples and/or the OpenAI API
documentation for more information on defining JSON schemas}

\item{schema_strict}{(optional) If TRUE, the schema will be strictly enforced.
This option is passed the the API when using 'openai' and to the 'jsonvalidate::json_validate()'
function when using 'text-based'}

\item{schema_in_prompt_as}{(optional) If providing a schema, and when using "text-based" or "ollama",
this argument specifies how the schema should be included in the prompt. If "example" (default),
the schema will be included as an example JSON object. If "schema", the schema will be included
as a JSON schema. "example" typically appears to work better}
}
\value{
A \code{\link[=tidyprompt]{tidyprompt()}} with an added \code{\link[=prompt_wrap]{prompt_wrap()}} which will ensure
that the LLM response is a valid JSON object
}
\description{
This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
The function can work with all models and providers when using type "text-based",
but also supports native settings for the OpenAI API and Ollama (the latter
being more efficient and powerful, though specific certain providers and models).
For type "text-based" and "ollama", the JSON schema will be validated
within the extraction function using the \href{https://cran.r-project.org/web/packages/jsonvalidate/index.html}{jsonvalidate} package. For "openai",
the schema will be added to the API request parameters and the API will ensure
the response matches the schema.
}
\examples{
base_prompt <- "How can I solve 8x + 7 = -23?"

#### Enforcing JSON without a schema: ####

\dontrun{
  # Text-based (works with any model and all providers):
  #   Adds request for JSON object to the prompt
  #   Extracts the JSON objects from the LLM response
  json1 <- base_prompt |>
    answer_as_json() |>
    send_prompt(llm_provider_ollama())

  # Ollama-type without schema
  #   Sets Ollama's 'format' parameter to 'json', enforcing JSON
  #   Adds request to prompt for a JSON object, as is recommended in documentation
  json2 <- base_prompt |>
    answer_as_json(type = "ollama") |>
    send_prompt(llm_provider_ollama())

  # OpenAI-type without schema
  #   Sets OpenAI's 'response_format' parameter to 'json_object', enforcing JSON
  #   Adds request to prompt for a JSON object, as is required by the API
  json3 <- base_prompt |>
    answer_as_json(type = "openai") |>
    send_prompt(llm_provider_openai())
}



#### Enforcing JSON with a schema: ####

# Make a list representing a JSON schema,
#   which the LLM response must adhere to:
json_schema <- list(
  name = "steps_to_solve", # Required for OpenAI API
  description = NULL, # Optional for OpenAI API
  schema = list(
    type = "object",
    properties = list(
      steps = list(
        type = "array",
        items = list(
          type = "object",
          properties = list(
            explanation = list(type = "string"),
            output = list(type = "string")
          ),
          required = c("explanation", "output"),
          additionalProperties = FALSE
        )
      ),
      final_answer = list(type = "string")
    ),
    required = c("steps", "final_answer"),
    additionalProperties = FALSE
  )
  # 'strict' parameter is set as argument 'answer_as_json()'
)

# Generate example R object based on schema:
generate_json_example_from_schema(json_schema)

\dontrun{
  # Text-based with schema
  #   Adds request for JSON & example based on schema to the prompt;
  #   extracts JSON and validates against the schema with
  #   the 'jsonvalidate' package
  json4 <- base_prompt |>
    answer_as_json(schema = json_schema) |>
    send_prompt(llm_provider_ollama())

  # Ollama with schema
  #   Sets Ollama's 'format' parameter to 'json', enforcing JSON
  #   Adds example based on schema to the prompt;
  #   extracts JSON and validates against the schema with
  #   the 'jsonvalidate' package
  json5 <- base_prompt |>
    answer_as_json(type = "ollama", schema = json_schema) |>
    send_prompt(llm_provider_ollama())

  # OpenAI with schema
  #   Sets OpenAI's 'response_format' parameter to 'json_schema',
  #   and adds the json_schema to the API request;
  #   as such, the API will natively enforce the schema
  json6 <- base_prompt |>
    answer_as_json(type = "openai", schema = json_schema) |>
    send_prompt(llm_provider_openai())
}
}
\seealso{
Other pre_built_prompt_wraps: 
\code{\link{add_text}()},
\code{\link{add_tools}()},
\code{\link{answer_as_boolean}()},
\code{\link{answer_as_code}()},
\code{\link{answer_as_integer}()},
\code{\link{answer_as_list}()},
\code{\link{answer_as_named_list}()},
\code{\link{answer_as_regex}()},
\code{\link{answer_by_chain_of_thought}()},
\code{\link{answer_by_react}()},
\code{\link{prompt_wrap}()},
\code{\link{quit_if}()},
\code{\link{set_system_prompt}()}

Other answer_as_prompt_wraps: 
\code{\link{answer_as_boolean}()},
\code{\link{answer_as_code}()},
\code{\link{answer_as_integer}()},
\code{\link{answer_as_list}()},
\code{\link{answer_as_named_list}()},
\code{\link{answer_as_regex}()}

Other answer_as_json: 
\code{\link{generate_json_example_from_schema}()}
}
\concept{answer_as_json}
\concept{answer_as_prompt_wraps}
\concept{pre_built_prompt_wraps}
