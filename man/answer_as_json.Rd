% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/answer_as_json.R
\name{answer_as_json}
\alias{answer_as_json}
\title{Make LLM answer as JSON}
\usage{
answer_as_json(
  prompt,
  type = c("text-based", "ollama", "openai"),
  schema = NULL,
  schema_strict = FALSE,
  schema_in_prompt_as = c("example", "schema")
)
}
\arguments{
\item{prompt}{A single string or a \code{\link[=tidyprompt]{tidyprompt()}} object}

\item{type}{The way in which the JSON response will be enforced. Must be
one of "text-based", "ollama", or "openai".
\itemize{
\item "text-based": The JSON response will be extracted from the LLM response
and validated within an extraction function.
\item "openai": Relevant parameters will be added to the prompt through which
the API will enforce that the response is a valid JSON object. When providing
a schema, the 'response_format' parameter will be set to 'json_schema' and the
schema will also be added (enforcing a JSON response according to the schema).
When not providing a schema, the 'response_format' parameter will be set to 'json_object'
which enforces a JSON object response (but not any specific schema). For the latter,
a request for a JSON object is added to the prompt as is required by the OpenAI API.
\item "ollama": The parameter 'format' will be set to 'json' in the API request
(enforcing a JSON response). Besides that, the handling is the same as "text-based".
As recommended by Ollama, a request for a JSON object is added to the prompt.
}
"text-based" is more generally applicable, while "openai" may be more efficient but
specific to the conditions of the API (note that APIs besides the OpenAI API
may follow the same structure, in which case "openai" may also be used for those APIs)}

\item{schema}{(optional) A list representing a JSON schema object that the response must match.
\itemize{
\item When using type "text-based" or "ollama", the schema will be
added to the original prompt (see argument: 'schema_in_prompt_as'),
and the response will be validated against the schema with
\code{\link[jsonvalidate:json_validate]{jsonvalidate::json_validate()}}.
\item When using "openai", the schema will be added to the API request parameters
and the API will ensure the response matches the schema.
}
See example and/or the OpenAI API documentation for more information on defining JSON schemas}

\item{schema_strict}{If TRUE, the provided schema will be strictly enforced.
This option is passed the the API when using type "openai" and to the \code{\link[jsonvalidate:json_validate]{jsonvalidate::json_validate()}}
function when using "text-based" or "ollama". Default is FALSE (as it
is in the OpenAI API)}

\item{schema_in_prompt_as}{(optional) If providing a schema, and when using type
"text-based" or "ollama", this argument specifies how the schema should be included in the prompt.
\itemize{
\item "example" (default): The schema will be included as an example JSON object.
\item "schema": The schema will be included as a JSON schema.
}
"example" appears to work better in practice}
}
\value{
A \code{\link[=tidyprompt]{tidyprompt()}} with an added \code{\link[=prompt_wrap]{prompt_wrap()}} which will ensure
that the LLM response is a valid JSON object
}
\description{
This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.

The function can work with all models and providers when using type "text-based",
but also supports native settings for the OpenAI-type APIs and the Ollama API.

When providing a schema, for type "text-based" and "ollama", the JSON schema will be validated
within the extraction function using the \href{https://cran.r-project.org/web/packages/jsonvalidate/index.html}{jsonvalidate} package.
For type "openai", the schema will be added to the API request parameters and the API will
then ensure the response matches the schema.
}
\examples{
base_prompt <- "How can I solve 8x + 7 = -23?"

#### Enforcing JSON without a schema: ####

\dontrun{
  # Text-based (works with any model and all providers):
  #   Adds request for JSON object to the prompt
  #   Extracts the JSON objects from the LLM response
  json1 <- base_prompt |>
    answer_as_json() |>
    send_prompt(llm_provider_ollama())

  # Ollama-type without schema
  #   Sets Ollama's 'format' parameter to 'json', enforcing JSON
  #   Adds request to prompt for a JSON object, as is recommended in documentation
  json2 <- base_prompt |>
    answer_as_json(type = "ollama") |>
    send_prompt(llm_provider_ollama())

  # OpenAI-type without schema
  #   Sets OpenAI's 'response_format' parameter to 'json_object', enforcing JSON
  #   Adds request to prompt for a JSON object, as is required by the API
  json3 <- base_prompt |>
    answer_as_json(type = "openai") |>
    send_prompt(llm_provider_openai())
}



#### Enforcing JSON with a schema: ####

# Make a list representing a JSON schema,
#   which the LLM response must adhere to:
json_schema <- list(
  name = "steps_to_solve", # Required for OpenAI API
  description = NULL, # Optional for OpenAI API
  schema = list(
    type = "object",
    properties = list(
      steps = list(
        type = "array",
        items = list(
          type = "object",
          properties = list(
            explanation = list(type = "string"),
            output = list(type = "string")
          ),
          required = c("explanation", "output"),
          additionalProperties = FALSE
        )
      ),
      final_answer = list(type = "string")
    ),
    required = c("steps", "final_answer"),
    additionalProperties = FALSE
  )
  # 'strict' parameter is set as argument 'answer_as_json()'
)

# Generate example R object based on schema:
generate_json_example_from_schema(json_schema)

\dontrun{
  # Text-based with schema
  #   Adds request for JSON & example based on schema to the prompt;
  #   extracts JSON and validates against the schema with
  #   the 'jsonvalidate' package
  json4 <- base_prompt |>
    answer_as_json(schema = json_schema) |>
    send_prompt(llm_provider_ollama())

  # Ollama with schema
  #   Sets Ollama's 'format' parameter to 'json', enforcing JSON
  #   Adds example based on schema to the prompt;
  #   extracts JSON and validates against the schema with
  #   the 'jsonvalidate' package
  json5 <- base_prompt |>
    answer_as_json(type = "ollama", schema = json_schema) |>
    send_prompt(llm_provider_ollama())

  # OpenAI with schema
  #   Sets OpenAI's 'response_format' parameter to 'json_schema',
  #   and adds the json_schema to the API request;
  #   as such, the API will natively enforce the schema
  json6 <- base_prompt |>
    answer_as_json(type = "openai", schema = json_schema) |>
    send_prompt(llm_provider_openai())
}
}
\seealso{
Other pre_built_prompt_wraps: 
\code{\link{add_text}()},
\code{\link{add_tools}()},
\code{\link{answer_as_boolean}()},
\code{\link{answer_as_code}()},
\code{\link{answer_as_integer}()},
\code{\link{answer_as_list}()},
\code{\link{answer_as_named_list}()},
\code{\link{answer_as_regex}()},
\code{\link{answer_by_chain_of_thought}()},
\code{\link{answer_by_react}()},
\code{\link{prompt_wrap}()},
\code{\link{quit_if}()},
\code{\link{set_system_prompt}()}

Other answer_as_prompt_wraps: 
\code{\link{answer_as_boolean}()},
\code{\link{answer_as_code}()},
\code{\link{answer_as_integer}()},
\code{\link{answer_as_list}()},
\code{\link{answer_as_named_list}()},
\code{\link{answer_as_regex}()}

Other answer_as_json: 
\code{\link{generate_json_example_from_schema}()}
}
\concept{answer_as_json}
\concept{answer_as_prompt_wraps}
\concept{pre_built_prompt_wraps}
